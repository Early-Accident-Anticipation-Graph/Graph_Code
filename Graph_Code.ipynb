{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba2af69",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/test/i3d_feat/testing/positive/clip_0014.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# all_data = np.load('./data/dad/i3d_feat/testing/negative/000831.npy')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# frames\u001b[39;00m\n\u001b[1;32m      6\u001b[0m all_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/dad/i3d_feat/testing/negative/000831.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m all_data1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/test/i3d_feat/testing/positive/clip_0014.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# all_data = np.load(f\"{feature_path}\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# all_feat = torch.from_numpy(all_data['data'])[:, 1:, :]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# all_bbox = torch.from_numpy(all_data['det']).float()  #(x1, y1, x2, y2, cls, accident/no acc)bottom left and top right coordinates\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# curr_vid_label = int(all_data['labels'][1])  \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_data\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    388\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    391\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/test/i3d_feat/testing/positive/clip_0014.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# all_data = np.load('./data/dad/i3d_feat/testing/negative/000831.npy')\n",
    "# frames\n",
    "all_data = np.load(f\"{'./data/dad/i3d_feat/testing/negative/000831.npy'}\")\n",
    "all_data1 = np.load(f\"{'./data/test/i3d_feat/testing/positive/clip_0014.npy'}\")\n",
    "\n",
    "\n",
    "\n",
    "# all_data = np.load(f\"{feature_path}\")\n",
    "# all_feat = torch.from_numpy(all_data['data'])[:, 1:, :]\n",
    "# all_bbox = torch.from_numpy(all_data['det']).float()  #(x1, y1, x2, y2, cls, accident/no acc)bottom left and top right coordinates\n",
    "\n",
    "# curr_vid_label = int(all_data['labels'][1])  \n",
    "\n",
    "print(all_data.dtype)\n",
    "print(\"all_data\")\n",
    "print(all_data1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89eba468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models import *\n",
    "#from dataset_dad import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "import scipy.io as io\n",
    "\n",
    "import sklearn \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from eval_utils import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1162b471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13d3c9bb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)   #3407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619b99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Classification criterion\n",
    "cls_criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "best_ap = -1 \n",
    "n_frames = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e69f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(epoch, model, test_dataloader):\n",
    "\n",
    "\t\"\"\" Function to evaluate the model on the test data \n",
    "\tInputs: \n",
    "\tepoch: training epoch number (0 if it is only testing)\n",
    "\tmodel: trained model \n",
    "\ttest_dataloader: Dataloader for the testset \n",
    "\t\"\"\"\n",
    "\n",
    "\tglobal best_ap\n",
    "\tmodel.eval()\n",
    "\ttotal_correct, total, all_toa = 0, 0, []\n",
    " \n",
    "\tfor batch_i, (X, edge_index, y_true, img_feat, video_adj_list, edge_embeddings, temporal_adj_list, obj_vis_feat, batch_vec, toa) in enumerate(test_dataloader):\n",
    "\t\tprint(\"Batch: \", batch_i)\n",
    "\t\tX = X.reshape(-1, X.shape[2])\n",
    "\t\timg_feat = img_feat.reshape(-1, img_feat.shape[2])\n",
    "\t\tedge_index = edge_index.reshape(-1, edge_index.shape[2])\n",
    "\t\tedge_embeddings = edge_embeddings.view(-1, edge_embeddings.shape[-1])\n",
    "\t\tvideo_adj_list = video_adj_list.reshape(-1, video_adj_list.shape[2])\n",
    "\t\ttemporal_adj_list = temporal_adj_list.reshape(-1, temporal_adj_list.shape[2])\n",
    "\t\ty = y_true.reshape(-1) \n",
    "\t\t\n",
    "\t\tobj_vis_feat = obj_vis_feat.reshape(-1, obj_vis_feat.shape[-1]).to(device)\n",
    "\t\tfeat_sim = pairwise_cosine_similarity(obj_vis_feat+1e-7, obj_vis_feat+1e-7)\n",
    "\t\ttemporal_edge_w = feat_sim[temporal_adj_list[0, :], temporal_adj_list[1, :]]\n",
    "\t\tbatch_vec = batch_vec.view(-1).long()\n",
    "\n",
    "\t\tX, edge_index, y, img_feat, video_adj_list = X.to(device), edge_index.to(device), y.to(device), img_feat.to(device), video_adj_list.to(device)\n",
    "\t\ttemporal_adj_list, temporal_edge_w, edge_embeddings, batch_vec = temporal_adj_list.to(device), temporal_edge_w.to(device), edge_embeddings.to(device), batch_vec.to(device)\n",
    "\t\tall_toa += [toa.item()]\n",
    "        \n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tlogits, probs = model(X, edge_index, img_feat, video_adj_list, edge_embeddings, temporal_adj_list, temporal_edge_w, batch_vec)\n",
    "\t\t\n",
    "\t\tpred_labels = probs.argmax(1)\n",
    "\t\t\n",
    "\t\ttotal_correct += (pred_labels == y).cpu().numpy().sum()\n",
    "\t\ttotal += y.shape[0]\n",
    "\t\t\n",
    "\t\tif batch_i == 0: \n",
    "\t\t\tall_probs_vid2 = probs[:, 1].cpu().unsqueeze(0)\n",
    "\t\t\tall_pred = pred_labels.cpu()\n",
    "\t\t\tall_y =  y.cpu() #.unsqueeze(0)\n",
    "\t\t\tall_y_vid =  torch.max(y).unsqueeze(0).cpu() #y.cpu() #.unsqueeze(0)\n",
    "\t\telse: \n",
    "\t\t\tall_probs_vid2 = torch.cat((all_probs_vid2, probs[:, 1].cpu().unsqueeze(0)))\n",
    "\t\t\tall_pred = torch.cat((all_pred, pred_labels.cpu()))\n",
    "\t\t\tall_y = torch.cat((all_y, y.cpu()))\n",
    "\t\t\tall_y_vid = torch.cat((all_y_vid, torch.max(y).unsqueeze(0).cpu()))\n",
    "\n",
    "\t\t# Empty cache\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\ttorch.cuda.empty_cache()\n",
    "\tprint(\"All toa :\", all_toa)\n",
    "\t#Print the avergae precision \n",
    "\tprint(\"All Probs vid2 : \", len(all_probs_vid2))\n",
    "\tprint(\"All y vid : \", len(all_y_vid))\n",
    "\treturn all_probs_vid2, all_y_vid, all_toa\n",
    "\t\n",
    "\tavg_prec, curr_ttc, _ = evaluation(all_probs_vid2.numpy(), all_y_vid.numpy(), all_toa)    \n",
    "\tavg_prec = 100 * avg_prec\n",
    "\n",
    "\t#Print the confusion matrix \n",
    "\tcf = confusion_matrix(all_y.numpy(), all_pred.numpy())\n",
    "\tprint(cf)\n",
    "\n",
    "\t#class-wise accuracy \n",
    "\tclass_recall = cf.diagonal()/cf.sum(axis=1)\n",
    "\tprint(np.round(class_recall, 3))\n",
    "\n",
    "\tif bool(opt.test_only):\n",
    "\t\texit(0)\n",
    "\n",
    "\t#Saving checkpoint\n",
    "\tif avg_prec > best_ap:\n",
    "\t\tbest_ap = avg_prec\n",
    "\t\tos.makedirs(\"model_checkpoints/dad\", exist_ok=True)\n",
    "\t\ttorch.save(model.state_dict(), f\"model_checkpoints/dad/{model.__class__.__name__}_{epoch}.pth\")\n",
    "\t\tprint(f\"Saved the model checkpoint - model_checkpoints/dad/{model.__class__.__name__}_{epoch}.pth\")\n",
    "\tprint(\"Best Frame avg precision: %.2f%%\" % (best_ap))\n",
    "\n",
    "\tmodel.train()\n",
    "\tprint(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06692901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_dad import Dataset\n",
    "img_dataset_path = \"data/dad/i3d_feat\"\n",
    "dataset_path = \"data/dad/obj_feat\"\n",
    "split_path = \"splits_dad/\"\n",
    "obj_mapping_file = \"data/dad/obj_idx_to_labels.json\"\n",
    "ref_interval = 20\n",
    "video_batch_size = 1\n",
    "# Define training set\n",
    "train_dataset = Dataset(\n",
    "    img_dataset_path = img_dataset_path,\n",
    "    dataset_path=dataset_path,\n",
    "    split_path=split_path,\n",
    "    #frame_batch_size=opt.batch_size,\n",
    "    ref_interval=ref_interval,\n",
    "    objmap_file=obj_mapping_file,\n",
    "    training=True,\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=video_batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5313f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test set\n",
    "test_video_batch_size = 1\n",
    "test_dataset = Dataset(\n",
    "    img_dataset_path = img_dataset_path,\n",
    "    dataset_path=dataset_path,\n",
    "    split_path=split_path,\n",
    "    #frame_batch_size=opt.batch_size,\n",
    "    ref_interval=ref_interval,\n",
    "    objmap_file=obj_mapping_file,\n",
    "    training=False,\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_video_batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83585c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaceTempGoG_detr_dad(\n",
      "  (x_fc): Linear(in_features=4096, out_features=512, bias=True)\n",
      "  (x_bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (obj_l_fc): Linear(in_features=300, out_features=128, bias=True)\n",
      "  (obj_l_bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gc1_spatial): GCNConv(640, 128)\n",
      "  (gc1_norm1): InstanceNorm(128)\n",
      "  (gc1_temporal): GCNConv(640, 128)\n",
      "  (gc1_norm2): InstanceNorm(128)\n",
      "  (pool): TopKPooling(256, ratio=0.8, multiplier=1.0)\n",
      "  (img_fc): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (gc2_sg): GATv2Conv(256, 128, heads=1)\n",
      "  (gc2_norm1): InstanceNorm(128)\n",
      "  (gc2_i3d): GATv2Conv(512, 128, heads=1)\n",
      "  (gc2_norm2): InstanceNorm(128)\n",
      "  (classify_fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (classify_fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.2)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpaceTempGoG_detr_dad(\n",
       "  (x_fc): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (x_bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (obj_l_fc): Linear(in_features=300, out_features=128, bias=True)\n",
       "  (obj_l_bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gc1_spatial): GCNConv(640, 128)\n",
       "  (gc1_norm1): InstanceNorm(128)\n",
       "  (gc1_temporal): GCNConv(640, 128)\n",
       "  (gc1_norm2): InstanceNorm(128)\n",
       "  (pool): TopKPooling(256, ratio=0.8, multiplier=1.0)\n",
       "  (img_fc): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (gc2_sg): GATv2Conv(256, 128, heads=1)\n",
       "  (gc2_norm1): InstanceNorm(128)\n",
       "  (gc2_i3d): GATv2Conv(512, 128, heads=1)\n",
       "  (gc2_norm2): InstanceNorm(128)\n",
       "  (classify_fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (classify_fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.2)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\t# Define network\n",
    "input_dim = 4096\n",
    "embedding_dim = 256\n",
    "img_feat_dim = 2048\n",
    "num_classes = 2\n",
    "model = SpaceTempGoG_detr_dad(input_dim=input_dim, embedding_dim=embedding_dim, img_feat_dim=img_feat_dim, num_classes=num_classes).to(device)\n",
    "print(model)\n",
    "\t\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eacd8178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 3581698\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac272f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# Add weights from checkpoint model if specified\n",
    "checkpoint_model =\"model_checkpoints/dad_model.pth\"\n",
    "if checkpoint_model:\n",
    "    #model.load_state_dict(torch.load(checkpoint_model, map_location=torch.device('cpu')))\n",
    "    #model.load_state_dict(torch.load(checkpoint_model))\n",
    "    checkpoint = torch.load(checkpoint_model, map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d68a1eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(98173) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(98178) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(98181) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "python(98202) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "python(98224) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "python(98244) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "python(98266) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "python(98287) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "python(98308) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "python(98330) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_path:: data/dad/obj_feat/testing/b008_000927.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b031_001094.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b001_000860.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b015_000459.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b031_000561.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b004_000903.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b017_000556.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b035_001096.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b025_001008.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b006_000880.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b008_000866.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b010_000879.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b022_001020.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b024_000938.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b035_000589.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b021_000986.npz\n",
      "Batch:  0\n",
      "feature_path:: data/dad/obj_feat/testing/b040_001035.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b046_000510.npz\n",
      "Batch:  1\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b030_001017.npz 2\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b023_001016.npzBatch:  3\n",
      "\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b037_001081.npz\n",
      " 4\n",
      "Batch:  5\n",
      "feature_path:: data/dad/obj_feat/testing/b014_000897.npz\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b024_000520.npz\n",
      " 6\n",
      "feature_path:: data/dad/obj_feat/testing/b035_001073.npzBatch:  7\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b028_001013.npz\n",
      "Batch:  8\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b009_000887.npz\n",
      " 9\n",
      "feature_path:: data/dad/obj_feat/testing/b030_000982.npz\n",
      "Batch:  10\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b009_000469.npz\n",
      " 11\n",
      "feature_path:: data/dad/obj_feat/testing/b045_000580.npz\n",
      "Batch:  12\n",
      "feature_path:: data/dad/obj_feat/testing/b005_000472.npz\n",
      "Batch:  13\n",
      "feature_path:: data/dad/obj_feat/testing/b004_000873.npz\n",
      "Batch:  14\n",
      "feature_path:: data/dad/obj_feat/testing/b027_001005.npzBatch:  15\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b012_000925.npz\n",
      "Batch:  16\n",
      "feature_path:: data/dad/obj_feat/testing/b031_001042.npz\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "feature_path:: data/dad/obj_feat/testing/b033_000565.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b006_000900.npzBatch:  19\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b002_000489.npz\n",
      "Batch:  20\n",
      "feature_path:: data/dad/obj_feat/testing/b007_001130.npz\n",
      "Batch:  21\n",
      "feature_path:: data/dad/obj_feat/testing/b010_000899.npz\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "feature_path:: data/dad/obj_feat/testing/b009_000901.npz\n",
      "Batch:  24\n",
      "feature_path:: data/dad/obj_feat/testing/b039_000603.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b032_001103.npz\n",
      "Batch:  25\n",
      "feature_path:: data/dad/obj_feat/testing/b026_000522.npz\n",
      "Batch:  26\n",
      "feature_path:: data/dad/obj_feat/testing/b022_000544.npz\n",
      "Batch:  27\n",
      "feature_path:: data/dad/obj_feat/testing/b012_000466.npz\n",
      "Batch:  28\n",
      "feature_path:: data/dad/obj_feat/testing/b004_000837.npz\n",
      "Batch:  29\n",
      "feature_path:: data/dad/obj_feat/testing/b001_000490.npz\n",
      "Batch:  30\n",
      "feature_path:: data/dad/obj_feat/testing/b039_001057.npz\n",
      "Batch:  31\n",
      "feature_path:: data/dad/obj_feat/testing/b034_001079.npz\n",
      "Batch:  32\n",
      "feature_path:: data/dad/obj_feat/testing/b033_001055.npz\n",
      "Batch:  33\n",
      "feature_path:: data/dad/obj_feat/testing/b013_000892.npz\n",
      "Batch:  34\n",
      "feature_path:: data/dad/obj_feat/testing/b044_000594.npzBatch:  35\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b009_000918.npz\n",
      "Batch:  36\n",
      "feature_path:: data/dad/obj_feat/testing/b037_001067.npz\n",
      "Batch:  37\n",
      "feature_path:: data/dad/obj_feat/testing/b042_001064.npz\n",
      "Batch:  38\n",
      "feature_path:: data/dad/obj_feat/testing/b038_001082.npzBatch:  39\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b037_001051.npzBatch:  40\n",
      "\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b022_000515.npz\n",
      " 41\n",
      "feature_path:: data/dad/obj_feat/testing/b002_000835.npzBatch:  42\n",
      "\n",
      "Batch:  43\n",
      "feature_path:: data/dad/obj_feat/testing/b033_000585.npz\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b025_000955.npz 44\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b015_000831.npz\n",
      "Batch:  45\n",
      "feature_path:: data/dad/obj_feat/testing/b003_000856.npz\n",
      "Batch:  46\n",
      "feature_path:: data/dad/obj_feat/testing/b040_001039.npz\n",
      "Batch:  47\n",
      "feature_path:: data/dad/obj_feat/testing/b042_001088.npz\n",
      "Batch:  48\n",
      "feature_path:: data/dad/obj_feat/testing/b041_000587.npz\n",
      "Batch:  49\n",
      "feature_path:: data/dad/obj_feat/testing/b014_000908.npz\n",
      "Batch:  50\n",
      "feature_path:: data/dad/obj_feat/testing/b012_000883.npz\n",
      "Batch:  51\n",
      "feature_path:: data/dad/obj_feat/testing/b021_000530.npz\n",
      "Batch:  52\n",
      "feature_path:: data/dad/obj_feat/testing/b027_001010.npz\n",
      "Batch:  53\n",
      "feature_path:: data/dad/obj_feat/testing/b037_000584.npzBatch:  54\n",
      "\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b036_001091.npz 55\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b042_001117.npzBatch:  56\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b045_001040.npz\n",
      "Batch:  57\n",
      "feature_path:: data/dad/obj_feat/testing/b001_000502.npz\n",
      "Batch:  58\n",
      "feature_path:: data/dad/obj_feat/testing/b026_000514.npz\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "feature_path:: data/dad/obj_feat/testing/b040_001105.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b041_000567.npz\n",
      "Batch:  61\n",
      "feature_path:: data/dad/obj_feat/testing/b017_001028.npzBatch:  62\n",
      "\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b022_001022.npz 63\n",
      "\n",
      "Batch:  64\n",
      "feature_path:: data/dad/obj_feat/testing/b046_001129.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b005_000911.npz\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "feature_path:: data/dad/obj_feat/testing/b009_000463.npz\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b036_000579.npz 67\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b024_000988.npz\n",
      "Batch:  68\n",
      "feature_path:: data/dad/obj_feat/testing/b008_000858.npz\n",
      "Batch:  69\n",
      "feature_path:: data/dad/obj_feat/testing/b043_000588.npz\n",
      "Batch:  70\n",
      "feature_path:: data/dad/obj_feat/testing/b005_000917.npz\n",
      "Batch:  71\n",
      "feature_path:: data/dad/obj_feat/testing/b020_000941.npz\n",
      "Batch:  72\n",
      "feature_path:: data/dad/obj_feat/testing/b038_000576.npz\n",
      "Batch:  73\n",
      "feature_path:: data/dad/obj_feat/testing/b003_000877.npz\n",
      "Batch:  74\n",
      "feature_path:: data/dad/obj_feat/testing/b001_000882.npz\n",
      "Batch:  75\n",
      "feature_path:: data/dad/obj_feat/testing/b034_001029.npz\n",
      "Batch:  76\n",
      "feature_path:: data/dad/obj_feat/testing/b019_000936.npz\n",
      "Batch:  77\n",
      "feature_path:: data/dad/obj_feat/testing/b042_000578.npz\n",
      "Batch:  78\n",
      "feature_path:: data/dad/obj_feat/testing/b029_000943.npz\n",
      "Batch:  79\n",
      "feature_path:: data/dad/obj_feat/testing/b045_000583.npz\n",
      "Batch:  80\n",
      "feature_path:: data/dad/obj_feat/testing/b013_000496.npz\n",
      "Batch:  81\n",
      "feature_path:: data/dad/obj_feat/testing/b038_000606.npz\n",
      "Batch:  82\n",
      "feature_path:: data/dad/obj_feat/testing/b005_000905.npz\n",
      "Batch:  83\n",
      "feature_path:: data/dad/obj_feat/testing/b030_000524.npz\n",
      "Batch:  84\n",
      "feature_path:: data/dad/obj_feat/testing/b033_001062.npz\n",
      "Batch:  85\n",
      "feature_path:: data/dad/obj_feat/testing/b004_000861.npz\n",
      "Batch:  86\n",
      "feature_path:: data/dad/obj_feat/testing/b017_000536.npzBatch:  87\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b016_000511.npzBatch:  88\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b028_000981.npz\n",
      "Batch:  89\n",
      "feature_path:: data/dad/obj_feat/testing/b022_000954.npz\n",
      "Batch:  90\n",
      "feature_path:: data/dad/obj_feat/testing/b035_001052.npz\n",
      "Batch:  91\n",
      "feature_path:: data/dad/obj_feat/testing/b033_000600.npz\n",
      "Batch:  92\n",
      "feature_path:: data/dad/obj_feat/testing/b036_000562.npzBatch:  93\n",
      "\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b003_000503.npz\n",
      " 94\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b033_000569.npz\n",
      " 95\n",
      "feature_path:: data/dad/obj_feat/testing/b005_000486.npz\n",
      "Batch:  96\n",
      "feature_path:: data/dad/obj_feat/testing/b029_000998.npzBatch:  97\n",
      "\n",
      "Batch:  98\n",
      "feature_path:: data/dad/obj_feat/testing/b041_000591.npz\n",
      "Batch:  99\n",
      "feature_path:: data/dad/obj_feat/testing/b024_000525.npz\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b033_000597.npz 100\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b031_001058.npz\n",
      "Batch:  101\n",
      "feature_path:: data/dad/obj_feat/testing/b028_000945.npz\n",
      "Batch:  102\n",
      "feature_path:: data/dad/obj_feat/testing/b004_000468.npz\n",
      "Batch:  103\n",
      "feature_path:: data/dad/obj_feat/testing/b021_000516.npz\n",
      "Batch:  104\n",
      "Batch:  105\n",
      "feature_path:: data/dad/obj_feat/testing/b009_000844.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b006_000906.npzBatch:  106\n",
      "\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b021_001000.npz 107\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b026_000967.npzBatch:  108\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b011_000909.npz\n",
      "Batch:  109\n",
      "feature_path:: data/dad/obj_feat/testing/b040_000572.npz\n",
      "Batch:  110\n",
      "Batch:  111\n",
      "feature_path:: data/dad/obj_feat/testing/b012_000506.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b015_000475.npz\n",
      "Batch:  112\n",
      "feature_path:: data/dad/obj_feat/testing/b020_000527.npz\n",
      "Batch:  113\n",
      "Batch:  114\n",
      "feature_path:: data/dad/obj_feat/testing/b029_000539.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b043_000592.npzBatch:  115\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b013_000910.npz\n",
      "Batch:  116\n",
      "Batch:  117\n",
      "feature_path:: data/dad/obj_feat/testing/b014_000843.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b019_000949.npzBatch:  118\n",
      "\n",
      "Batch:  119\n",
      "feature_path:: data/dad/obj_feat/testing/b017_001014.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b028_000528.npz\n",
      "Batch:  120\n",
      "feature_path:: data/dad/obj_feat/testing/b024_000959.npz\n",
      "Batch:  121\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b012_000838.npz 122\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b018_000940.npz\n",
      "Batch:  123\n",
      "feature_path:: data/dad/obj_feat/testing/b017_000942.npz\n",
      "Batch:  124\n",
      "feature_path:: data/dad/obj_feat/testing/b011_000845.npz\n",
      "Batch:  125\n",
      "feature_path:: data/dad/obj_feat/testing/b018_000509.npz\n",
      "Batch:  126\n",
      "feature_path:: data/dad/obj_feat/testing/b006_000859.npz\n",
      "Batch:  127\n",
      "feature_path:: data/dad/obj_feat/testing/b027_000968.npz\n",
      "Batch:  128\n",
      "feature_path:: data/dad/obj_feat/testing/b008_000460.npz\n",
      "Batch:  129\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b007_000461.npz\n",
      " 130\n",
      "feature_path:: data/dad/obj_feat/testing/b034_001118.npz\n",
      "Batch:  131\n",
      "feature_path:: data/dad/obj_feat/testing/b018_000507.npz\n",
      "Batch:  132\n",
      "feature_path:: data/dad/obj_feat/testing/b012_000465.npz\n",
      "Batch:  133\n",
      "feature_path:: data/dad/obj_feat/testing/b003_000916.npz\n",
      "Batch:  134\n",
      "feature_path:: data/dad/obj_feat/testing/b046_000508.npz\n",
      "Batch:  135\n",
      "feature_path:: data/dad/obj_feat/testing/b046_000512.npz\n",
      "Batch:  136\n",
      "feature_path:: data/dad/obj_feat/testing/b021_000974.npz\n",
      "Batch:  137\n",
      "feature_path:: data/dad/obj_feat/testing/b008_000484.npz\n",
      "Batch:  138\n",
      "feature_path:: data/dad/obj_feat/testing/b020_000550.npz\n",
      "Batch:  139\n",
      "feature_path:: data/dad/obj_feat/testing/b019_000992.npzBatch:  140\n",
      "\n",
      "Batch:  141\n",
      "feature_path:: data/dad/obj_feat/testing/b025_000979.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b042_001100.npz\n",
      "Batch:  142\n",
      "Batch:  143\n",
      "feature_path:: data/dad/obj_feat/testing/b046_000513.npz\n",
      "Batch:  144\n",
      "feature_path:: data/dad/obj_feat/testing/b001_000904.npz\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b043_001043.npz\n",
      " 145\n",
      "feature_path:: data/dad/obj_feat/testing/b014_000878.npz\n",
      "Batch:  146\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b009_000846.npz 147\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b020_001018.npz\n",
      "Batch:  148\n",
      "feature_path:: data/dad/obj_feat/testing/b029_000957.npz\n",
      "Batch:  149\n",
      "feature_path:: data/dad/obj_feat/testing/b021_001024.npz\n",
      "Batch:  150\n",
      "Batch:  151\n",
      "feature_path:: data/dad/obj_feat/testing/b023_000994.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b032_000557.npz\n",
      "Batch:  152\n",
      "feature_path:: data/dad/obj_feat/testing/b011_000483.npzBatch:  153\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b007_000493.npz\n",
      "Batch:  154\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b029_000997.npz 155\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b008_000833.npz\n",
      "Batch:  156\n",
      "feature_path:: data/dad/obj_feat/testing/b043_001063.npz\n",
      "Batch:  157\n",
      "feature_path:: data/dad/obj_feat/testing/b030_000548.npz\n",
      "Batch:  158\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b012_000871.npz\n",
      " 159\n",
      "feature_path:: data/dad/obj_feat/testing/b030_000541.npz\n",
      "Batch:  160\n",
      "feature_path:: data/dad/obj_feat/testing/b035_001077.npz\n",
      "Batch:  161\n",
      "feature_path:: data/dad/obj_feat/testing/b010_000923.npz\n",
      "Batch:  162\n",
      "feature_path:: data/dad/obj_feat/testing/b004_000478.npz\n",
      "Batch:  163\n",
      "feature_path:: data/dad/obj_feat/testing/b034_000564.npz\n",
      "Batch:  164\n",
      "feature_path:: data/dad/obj_feat/testing/b007_000457.npz\n",
      "Batch:  165\n",
      "feature_path:: data/dad/obj_feat/testing/b042_001031.npz\n",
      "Batch:  166\n",
      "feature_path:: data/dad/obj_feat/testing/b013_000928.npz\n",
      "Batch:  167\n",
      "feature_path:: data/dad/obj_feat/testing/b041_001127.npz\n",
      "Batch:  168\n",
      "feature_path:: data/dad/obj_feat/testing/b043_000568.npzBatch:  169\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b015_000875.npz\n",
      "Batch:  170\n",
      "feature_path:: data/dad/obj_feat/testing/b034_001080.npzBatch:  171\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b029_000976.npz\n",
      "Batch:  172\n",
      "feature_path:: data/dad/obj_feat/testing/b006_000853.npz\n",
      "Batch:  173\n",
      "feature_path:: data/dad/obj_feat/testing/b024_000958.npz\n",
      "Batch:  174\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b038_001098.npz\n",
      " 175\n",
      "feature_path:: data/dad/obj_feat/testing/b010_000497.npz\n",
      "Batch:  176\n",
      "feature_path:: data/dad/obj_feat/testing/b026_001023.npz\n",
      "Batch:  177\n",
      "feature_path:: data/dad/obj_feat/testing/b030_000526.npz\n",
      "Batch:  178\n",
      "feature_path:: data/dad/obj_feat/testing/b033_001126.npz\n",
      "Batch:  179\n",
      "feature_path:: data/dad/obj_feat/testing/b032_001053.npz\n",
      "Batch:  180\n",
      "feature_path:: data/dad/obj_feat/testing/b026_001001.npz\n",
      "Batch:  181\n",
      "feature_path:: data/dad/obj_feat/testing/b019_000519.npz\n",
      "Batch:  182\n",
      "feature_path:: data/dad/obj_feat/testing/b029_000523.npz\n",
      "Batch:  183\n",
      "feature_path:: data/dad/obj_feat/testing/b040_001119.npz\n",
      "Batch:  184\n",
      "feature_path:: data/dad/obj_feat/testing/b037_000596.npz\n",
      "Batch:  185\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b036_001089.npz\n",
      " 186\n",
      "feature_path:: data/dad/obj_feat/testing/b036_001032.npz\n",
      "Batch:  187\n",
      "feature_path:: data/dad/obj_feat/testing/b041_001046.npz\n",
      "Batch:  188\n",
      "feature_path:: data/dad/obj_feat/testing/b028_001002.npz\n",
      "Batch:  189\n",
      "feature_path:: data/dad/obj_feat/testing/b033_000598.npz\n",
      "Batch:  190\n",
      "feature_path:: data/dad/obj_feat/testing/b008_000884.npz\n",
      "Batch:  191\n",
      "feature_path:: data/dad/obj_feat/testing/b034_001071.npz\n",
      "Batch:  192\n",
      "feature_path:: data/dad/obj_feat/testing/b023_000951.npz\n",
      "Batch:  193\n",
      "feature_path:: data/dad/obj_feat/testing/b038_001110.npz\n",
      "Batch:  194\n",
      "feature_path:: data/dad/obj_feat/testing/b022_000980.npz\n",
      "Batch:  195\n",
      "feature_path:: data/dad/obj_feat/testing/b010_000924.npz\n",
      "Batch:  196\n",
      "feature_path:: data/dad/obj_feat/testing/b036_001122.npz\n",
      "Batch:  197\n",
      "feature_path:: data/dad/obj_feat/testing/b006_000870.npz\n",
      "Batch:  198\n",
      "feature_path:: data/dad/obj_feat/testing/b019_000546.npz\n",
      "Batch:  199\n",
      "feature_path:: data/dad/obj_feat/testing/b045_001087.npz\n",
      "Batch:  200\n",
      "Batch:  201\n",
      "feature_path:: data/dad/obj_feat/testing/b037_001120.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b031_001068.npz\n",
      "Batch:  202\n",
      "feature_path:: data/dad/obj_feat/testing/b046_000520.npz\n",
      "Batch:  203\n",
      "feature_path:: data/dad/obj_feat/testing/b043_001092.npzBatch:  204\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b030_000939.npz\n",
      "Batch:  205\n",
      "feature_path:: data/dad/obj_feat/testing/b003_000863.npz\n",
      "Batch:  206\n",
      "feature_path:: data/dad/obj_feat/testing/b028_000946.npz\n",
      "Batch:  207\n",
      "feature_path:: data/dad/obj_feat/testing/b039_001107.npz\n",
      "Batch:  208\n",
      "feature_path:: data/dad/obj_feat/testing/b026_001015.npz\n",
      "Batch:  209\n",
      "feature_path:: data/dad/obj_feat/testing/b019_000962.npz\n",
      "Batch:  210\n",
      "feature_path:: data/dad/obj_feat/testing/b030_000532.npz\n",
      "Batch:  211\n",
      "feature_path:: data/dad/obj_feat/testing/b027_000518.npz\n",
      "Batch:  212\n",
      "feature_path:: data/dad/obj_feat/testing/b011_000482.npz\n",
      "Batch:  213\n",
      "feature_path:: data/dad/obj_feat/testing/b030_001009.npz\n",
      "Batch:  214\n",
      "feature_path:: data/dad/obj_feat/testing/b043_000558.npz\n",
      "Batch:  215\n",
      "feature_path:: data/dad/obj_feat/testing/b015_000894.npz\n",
      "Batch:  216\n",
      "feature_path:: data/dad/obj_feat/testing/b045_001070.npz\n",
      "Batch:  217\n",
      "feature_path:: data/dad/obj_feat/testing/b007_000885.npz\n",
      "Batch:  218\n",
      "feature_path:: data/dad/obj_feat/testing/b023_000547.npz\n",
      "Batch:  219\n",
      "feature_path:: data/dad/obj_feat/testing/b039_000575.npz\n",
      "Batch:  220\n",
      "feature_path:: data/dad/obj_feat/testing/b004_000499.npz\n",
      "Batch:  221\n",
      "feature_path:: data/dad/obj_feat/testing/b041_001036.npzBatch:  222\n",
      "\n",
      "Batch:  223\n",
      "feature_path:: data/dad/obj_feat/testing/b033_001060.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b003_000470.npz\n",
      "Batch:  224\n",
      "feature_path:: data/dad/obj_feat/testing/b015_000913.npzBatch:  225\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b044_001085.npzBatch:  226\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b040_000574.npzBatch:  227\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b034_001095.npz\n",
      "Batch:  228\n",
      "feature_path:: data/dad/obj_feat/testing/b002_000500.npz\n",
      "Batch:  229\n",
      "feature_path:: data/dad/obj_feat/testing/b046_000517.npz\n",
      "Batch:  230\n",
      "feature_path:: data/dad/obj_feat/testing/b025_000529.npz\n",
      "Batch:  231\n",
      "feature_path:: data/dad/obj_feat/testing/b034_000599.npzBatch:  232\n",
      "\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b015_000481.npz\n",
      " 233\n",
      "feature_path:: data/dad/obj_feat/testing/b043_001041.npz\n",
      "Batch:  234\n",
      "feature_path:: data/dad/obj_feat/testing/b010_000868.npz\n",
      "Batch:  235\n",
      "feature_path:: data/dad/obj_feat/testing/b029_000953.npz\n",
      "Batch:  236\n",
      "feature_path:: data/dad/obj_feat/testing/b014_000862.npz\n",
      "Batch:  237\n",
      "feature_path:: data/dad/obj_feat/testing/b035_001047.npz\n",
      "Batch:  238\n",
      "feature_path:: data/dad/obj_feat/testing/b017_000944.npz\n",
      "Batch:  239\n",
      "feature_path:: data/dad/obj_feat/testing/b031_000582.npz\n",
      "Batch:  240\n",
      "feature_path:: data/dad/obj_feat/testing/b018_000508.npz\n",
      "Batch:  241\n",
      "feature_path:: data/dad/obj_feat/testing/b018_000553.npz\n",
      "Batch:  242\n",
      "feature_path:: data/dad/obj_feat/testing/b044_001050.npz\n",
      "Batch:  243\n",
      "feature_path:: data/dad/obj_feat/testing/b016_000540.npzBatch:  244\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b040_001074.npz\n",
      "Batch:  245\n",
      "feature_path:: data/dad/obj_feat/testing/b004_000488.npz\n",
      "Batch:  246\n",
      "feature_path:: data/dad/obj_feat/testing/b002_000847.npz\n",
      "Batch:  247\n",
      "feature_path:: data/dad/obj_feat/testing/b038_001097.npzBatch:  248\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b002_000474.npzBatch:  249\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b005_000458.npz\n",
      "Batch:  250\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b001_000865.npz\n",
      " 251\n",
      "feature_path:: data/dad/obj_feat/testing/b010_000852.npz\n",
      "Batch:  252\n",
      "feature_path:: data/dad/obj_feat/testing/b039_001056.npzBatch:  253\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b024_000964.npzBatch:  254\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b032_000571.npzBatch:  255\n",
      "\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b018_001006.npz\n",
      " 256\n",
      "feature_path:: data/dad/obj_feat/testing/b018_000995.npzBatch:  257\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b019_000937.npzBatch:  258\n",
      "\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b042_000590.npz 259\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b027_000993.npz\n",
      "Batch:  260\n",
      "feature_path:: data/dad/obj_feat/testing/b020_000512.npz\n",
      "Batch:  261\n",
      "feature_path:: data/dad/obj_feat/testing/b028_000931.npz\n",
      "Batch:  262\n",
      "feature_path:: data/dad/obj_feat/testing/b040_001048.npz\n",
      "Batch:  263\n",
      "feature_path:: data/dad/obj_feat/testing/b027_000960.npz\n",
      "Batch:  264\n",
      "feature_path:: data/dad/obj_feat/testing/b012_000921.npzBatch:  265\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b027_000991.npz\n",
      "Batch:  266\n",
      "feature_path:: data/dad/obj_feat/testing/b039_001113.npzBatch:  267\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b002_000498.npz\n",
      "Batch:  268\n",
      "feature_path:: data/dad/obj_feat/testing/b027_001011.npz\n",
      "Batch:  269\n",
      "feature_path:: data/dad/obj_feat/testing/b025_000973.npz\n",
      "Batch:  270\n",
      "feature_path:: data/dad/obj_feat/testing/b026_000983.npz\n",
      "Batch:  271\n",
      "feature_path:: data/dad/obj_feat/testing/b013_000477.npz\n",
      "Batch:  272\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b013_000886.npz\n",
      " 273\n",
      "Batch:  274\n",
      "feature_path:: data/dad/obj_feat/testing/b005_000851.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b011_000832.npz\n",
      "Batch:  275\n",
      "feature_path:: data/dad/obj_feat/testing/b031_000563.npz\n",
      "Batch:  276\n",
      "feature_path:: data/dad/obj_feat/testing/b035_000605.npz\n",
      "Batch:  277\n",
      "feature_path:: data/dad/obj_feat/testing/b011_000907.npz\n",
      "Batch:  278\n",
      "Batch:  279\n",
      "feature_path:: data/dad/obj_feat/testing/b038_001078.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b032_001049.npz\n",
      "Batch:  280\n",
      "feature_path:: data/dad/obj_feat/testing/b003_000467.npz\n",
      "Batch:  281\n",
      "feature_path:: data/dad/obj_feat/testing/b018_000552.npz\n",
      "Batch:  282\n",
      "feature_path:: data/dad/obj_feat/testing/b011_000505.npzBatch:  283\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b041_001128.npz\n",
      "Batch:  284\n",
      "feature_path:: data/dad/obj_feat/testing/b005_000898.npz\n",
      "Batch:  285\n",
      "Batch:  286\n",
      "feature_path:: data/dad/obj_feat/testing/b020_000948.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b042_001061.npzBatch:  287\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b014_000487.npz\n",
      "Batch:  288\n",
      "feature_path:: data/dad/obj_feat/testing/b022_000510.npz\n",
      "Batch:  289\n",
      "feature_path:: data/dad/obj_feat/testing/b045_001090.npz\n",
      "Batch:  290\n",
      "feature_path:: data/dad/obj_feat/testing/b019_000934.npz\n",
      "Batch:  291\n",
      "feature_path:: data/dad/obj_feat/testing/b021_000513.npz\n",
      "Batch:  292\n",
      "feature_path:: data/dad/obj_feat/testing/b012_000471.npz\n",
      "Batch:  293\n",
      "feature_path:: data/dad/obj_feat/testing/b017_000950.npz\n",
      "Batch:  294\n",
      "feature_path:: data/dad/obj_feat/testing/b012_000830.npz\n",
      "Batch:  295\n",
      "feature_path:: data/dad/obj_feat/testing/b028_001026.npz\n",
      "Batch:  296\n",
      "feature_path:: data/dad/obj_feat/testing/b044_001038.npz\n",
      "Batch:  297\n",
      "feature_path:: data/dad/obj_feat/testing/b040_001108.npz\n",
      "Batch:  298\n",
      "feature_path:: data/dad/obj_feat/testing/b007_000914.npz\n",
      "Batch:  299\n",
      "feature_path:: data/dad/obj_feat/testing/b020_000555.npz\n",
      "Batch:  300\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b032_001084.npz\n",
      " 301\n",
      "feature_path:: data/dad/obj_feat/testing/b035_000570.npz\n",
      "Batch:  302\n",
      "Batch:  303\n",
      "feature_path:: data/dad/obj_feat/testing/b017_001012.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b046_000511.npzBatch:  304\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b011_000485.npz\n",
      "Batch:  305\n",
      "feature_path:: data/dad/obj_feat/testing/b046_000516.npz\n",
      "Batch:  306\n",
      "feature_path:: data/dad/obj_feat/testing/b023_000952.npz\n",
      "Batch:  307\n",
      "feature_path:: data/dad/obj_feat/testing/b016_000975.npz\n",
      "Batch:  308\n",
      "feature_path:: data/dad/obj_feat/testing/b044_001106.npz\n",
      "Batch:  309\n",
      "feature_path:: data/dad/obj_feat/testing/b015_000872.npz\n",
      "Batch:  310\n",
      "feature_path:: data/dad/obj_feat/testing/b024_000935.npz\n",
      "Batch:  311\n",
      "feature_path:: data/dad/obj_feat/testing/b041_001121.npz\n",
      "Batch:  312\n",
      "feature_path:: data/dad/obj_feat/testing/b023_000521.npz\n",
      "Batch:  313\n",
      "feature_path:: data/dad/obj_feat/testing/b011_000464.npz\n",
      "Batch:  314\n",
      "feature_path:: data/dad/obj_feat/testing/b037_000601.npz\n",
      "Batch:  315\n",
      "feature_path:: data/dad/obj_feat/testing/b008_000850.npz\n",
      "Batch:  316\n",
      "feature_path:: data/dad/obj_feat/testing/b025_000932.npz\n",
      "Batch:  317\n",
      "feature_path:: data/dad/obj_feat/testing/b001_000834.npz\n",
      "Batch:  318\n",
      "feature_path:: data/dad/obj_feat/testing/b002_000836.npz\n",
      "Batch:  319\n",
      "feature_path:: data/dad/obj_feat/testing/b006_000874.npz\n",
      "Batch:  320\n",
      "feature_path:: data/dad/obj_feat/testing/b023_000987.npz\n",
      "Batch:  321\n",
      "feature_path:: data/dad/obj_feat/testing/b015_000476.npz\n",
      "Batch:  322\n",
      "feature_path:: data/dad/obj_feat/testing/b004_000895.npz\n",
      "Batch:  323\n",
      "feature_path:: data/dad/obj_feat/testing/b042_000602.npz\n",
      "Batch:  324\n",
      "feature_path:: data/dad/obj_feat/testing/b028_001007.npz\n",
      "Batch:  325\n",
      "feature_path:: data/dad/obj_feat/testing/b014_000890.npz\n",
      "Batch:  326\n",
      "feature_path:: data/dad/obj_feat/testing/b031_000604.npz\n",
      "Batch:  327\n",
      "feature_path:: data/dad/obj_feat/testing/b020_001027.npz\n",
      "Batch:  328\n",
      "feature_path:: data/dad/obj_feat/testing/b041_000577.npz\n",
      "Batch:  329\n",
      "feature_path:: data/dad/obj_feat/testing/b032_001083.npz\n",
      "Batch:  330\n",
      "feature_path:: data/dad/obj_feat/testing/b023_000538.npz\n",
      "Batch:  331\n",
      "feature_path:: data/dad/obj_feat/testing/b016_000977.npz\n",
      "Batch:  332\n",
      "feature_path:: data/dad/obj_feat/testing/b013_000494.npz\n",
      "Batch:  333\n",
      "feature_path:: data/dad/obj_feat/testing/b045_001099.npz\n",
      "Batch:  334\n",
      "feature_path:: data/dad/obj_feat/testing/b007_000479.npz\n",
      "Batch:  335\n",
      "feature_path:: data/dad/obj_feat/testing/b031_001104.npz\n",
      "Batch:  336\n",
      "feature_path:: data/dad/obj_feat/testing/b032_001125.npz\n",
      "Batch:  337\n",
      "feature_path:: data/dad/obj_feat/testing/b036_001059.npz\n",
      "Batch:  338\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b010_000492.npz 339\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b001_000848.npz\n",
      "Batch:  340\n",
      "feature_path:: data/dad/obj_feat/testing/b004_000501.npz\n",
      "Batch:  341\n",
      "feature_path:: data/dad/obj_feat/testing/b044_001076.npz\n",
      "Batch:  342\n",
      "feature_path:: data/dad/obj_feat/testing/b044_000560.npz\n",
      "Batch:  343\n",
      "feature_path:: data/dad/obj_feat/testing/b013_000841.npz\n",
      "Batch:  344\n",
      "feature_path:: data/dad/obj_feat/testing/b010_000840.npz\n",
      "Batch:  345\n",
      "feature_path:: data/dad/obj_feat/testing/b043_001030.npz\n",
      "Batch:  346\n",
      "feature_path:: data/dad/obj_feat/testing/b042_000593.npz\n",
      "Batch:  347\n",
      "feature_path:: data/dad/obj_feat/testing/b013_000915.npzBatch:  348\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b009_000480.npz\n",
      "Batch:  349\n",
      "feature_path:: data/dad/obj_feat/testing/b006_000857.npz\n",
      "Batch:  350\n",
      "feature_path:: data/dad/obj_feat/testing/b045_001045.npz\n",
      "Batch:  351\n",
      "feature_path:: data/dad/obj_feat/testing/b035_001111.npz\n",
      "Batch:  352\n",
      "feature_path:: data/dad/obj_feat/testing/b026_001019.npz\n",
      "Batch:  353\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b013_000855.npz\n",
      " 354\n",
      "feature_path:: data/dad/obj_feat/testing/b025_000970.npz\n",
      "Batch:  355\n",
      "feature_path:: data/dad/obj_feat/testing/b017_000999.npz\n",
      "Batch:  356\n",
      "feature_path:: data/dad/obj_feat/testing/b001_000867.npz\n",
      "Batch:  357\n",
      "feature_path:: data/dad/obj_feat/testing/b030_000551.npzBatch:  358\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b025_000554.npz\n",
      "Batch:  359\n",
      "feature_path:: data/dad/obj_feat/testing/b026_000533.npz\n",
      "Batch:  360\n",
      "feature_path:: data/dad/obj_feat/testing/b003_000881.npz\n",
      "Batch:  361\n",
      "feature_path:: data/dad/obj_feat/testing/b016_000978.npz\n",
      "Batch:  362\n",
      "feature_path:: data/dad/obj_feat/testing/b032_000586.npz\n",
      "Batch:  363\n",
      "feature_path:: data/dad/obj_feat/testing/b026_000542.npz\n",
      "Batch:  364\n",
      "feature_path:: data/dad/obj_feat/testing/b016_000929.npz\n",
      "Batch:  365\n",
      "feature_path:: data/dad/obj_feat/testing/b045_001044.npz\n",
      "Batch:  366\n",
      "feature_path:: data/dad/obj_feat/testing/b034_000559.npz\n",
      "Batch:  367\n",
      "feature_path:: data/dad/obj_feat/testing/b024_000985.npz\n",
      "Batch:  368\n",
      "feature_path:: data/dad/obj_feat/testing/b023_000545.npz\n",
      "Batch:  369\n",
      "feature_path:: data/dad/obj_feat/testing/b003_000495.npz\n",
      "Batch:  370\n",
      "feature_path:: data/dad/obj_feat/testing/b018_001021.npz\n",
      "Batch:  371\n",
      "feature_path:: data/dad/obj_feat/testing/b003_000912.npz\n",
      "Batch:  372\n",
      "feature_path:: data/dad/obj_feat/testing/b008_000462.npz\n",
      "Batch:  373\n",
      "feature_path:: data/dad/obj_feat/testing/b039_001123.npz\n",
      "Batch:  374\n",
      "feature_path:: data/dad/obj_feat/testing/b020_000972.npz\n",
      "Batch:  375\n",
      "feature_path:: data/dad/obj_feat/testing/b007_000902.npz\n",
      "Batch:  376\n",
      "feature_path:: data/dad/obj_feat/testing/b037_001037.npz\n",
      "Batch:  377\n",
      "feature_path:: data/dad/obj_feat/testing/b025_000984.npz\n",
      "Batch:  378\n",
      "feature_path:: data/dad/obj_feat/testing/b029_000531.npz\n",
      "Batch:  379\n",
      "feature_path:: data/dad/obj_feat/testing/b037_001101.npz\n",
      "Batch:  380\n",
      "feature_path:: data/dad/obj_feat/testing/b014_000889.npz\n",
      "Batch:  381\n",
      "feature_path:: data/dad/obj_feat/testing/b011_000473.npz\n",
      "Batch:  382\n",
      "feature_path:: data/dad/obj_feat/testing/b043_001033.npz\n",
      "Batch:  383\n",
      "feature_path:: data/dad/obj_feat/testing/b021_000947.npz\n",
      "Batch:  384\n",
      "feature_path:: data/dad/obj_feat/testing/b027_001003.npz\n",
      "Batch:  385\n",
      "feature_path:: data/dad/obj_feat/testing/b018_000971.npz\n",
      "Batch:  386\n",
      "feature_path:: data/dad/obj_feat/testing/b019_001004.npz\n",
      "Batch:  387\n",
      "feature_path:: data/dad/obj_feat/testing/b002_000926.npz\n",
      "Batch:  388\n",
      "feature_path:: data/dad/obj_feat/testing/b039_001112.npz\n",
      "Batch:  389\n",
      "feature_path:: data/dad/obj_feat/testing/b005_000854.npz\n",
      "Batch:  390\n",
      "feature_path:: data/dad/obj_feat/testing/b021_000933.npz\n",
      "Batch:  391\n",
      "feature_path:: data/dad/obj_feat/testing/b015_000849.npz\n",
      "Batch:  392\n",
      "feature_path:: data/dad/obj_feat/testing/b007_000919.npz\n",
      "Batch:  393\n",
      "Batch:  394\n",
      "feature_path:: data/dad/obj_feat/testing/b035_000581.npz\n",
      "feature_path:: data/dad/obj_feat/testing/b017_000930.npz\n",
      "Batch:  395\n",
      "feature_path:: data/dad/obj_feat/testing/b016_000534.npz\n",
      "Batch:  396\n",
      "feature_path:: data/dad/obj_feat/testing/b039_001086.npz\n",
      "Batch:  397\n",
      "feature_path:: data/dad/obj_feat/testing/b045_001114.npz\n",
      "Batch:  398\n",
      "feature_path:: data/dad/obj_feat/testing/b024_000969.npz\n",
      "Batch:  399\n",
      "feature_path:: data/dad/obj_feat/testing/b038_001109.npz\n",
      "Batch:  400\n",
      "feature_path:: data/dad/obj_feat/testing/b041_001069.npz\n",
      "Batch:  401\n",
      "feature_path:: data/dad/obj_feat/testing/b016_000966.npz\n",
      "Batch:  402\n",
      "feature_path:: data/dad/obj_feat/testing/b044_001065.npz\n",
      "Batch:  403\n",
      "feature_path:: data/dad/obj_feat/testing/b009_000896.npz\n",
      "Batch:  404\n",
      "feature_path:: data/dad/obj_feat/testing/b022_000989.npz\n",
      "Batch:  405\n",
      "Batch: feature_path:: data/dad/obj_feat/testing/b036_001054.npz\n",
      " 406\n",
      "feature_path:: data/dad/obj_feat/testing/b032_001124.npz\n",
      "Batch:  407\n",
      "feature_path:: data/dad/obj_feat/testing/b036_000566.npzBatch:  408\n",
      "\n",
      "feature_path:: data/dad/obj_feat/testing/b022_000517.npz\n",
      "Batch:  409\n",
      "feature_path:: data/dad/obj_feat/testing/b044_001034.npz\n",
      "Batch:  410\n",
      "feature_path:: data/dad/obj_feat/testing/b006_000876.npz\n",
      "Batch:  411\n",
      "feature_path:: data/dad/obj_feat/testing/b023_000537.npz\n",
      "Batch:  412\n",
      "feature_path:: data/dad/obj_feat/testing/b031_001116.npz\n",
      "Batch:  413\n",
      "feature_path:: data/dad/obj_feat/testing/b007_000491.npz\n",
      "Batch:  414\n",
      "feature_path:: data/dad/obj_feat/testing/b006_000920.npz\n",
      "Batch:  415\n",
      "feature_path:: data/dad/obj_feat/testing/b037_001066.npz\n",
      "Batch:  416\n",
      "feature_path:: data/dad/obj_feat/testing/b005_000864.npz\n",
      "Batch:  417\n",
      "feature_path:: data/dad/obj_feat/testing/b027_000963.npz\n",
      "Batch:  418\n",
      "feature_path:: data/dad/obj_feat/testing/b010_000842.npz\n",
      "Batch:  419\n",
      "feature_path:: data/dad/obj_feat/testing/b008_000504.npz\n",
      "Batch:  420\n",
      "feature_path:: data/dad/obj_feat/testing/b021_000961.npz\n",
      "Batch:  421\n",
      "feature_path:: data/dad/obj_feat/testing/b016_000990.npz\n",
      "Batch:  422\n",
      "feature_path:: data/dad/obj_feat/testing/b039_000595.npz\n",
      "Batch:  423\n",
      "feature_path:: data/dad/obj_feat/testing/b038_001115.npz\n",
      "Batch:  424\n",
      "feature_path:: data/dad/obj_feat/testing/b038_001072.npz\n",
      "Batch:  425\n",
      "feature_path:: data/dad/obj_feat/testing/b019_000535.npz\n",
      "Batch:  426\n",
      "feature_path:: data/dad/obj_feat/testing/b002_000893.npz\n",
      "Batch:  427\n",
      "feature_path:: data/dad/obj_feat/testing/b028_000956.npz\n",
      "Batch:  428\n",
      "feature_path:: data/dad/obj_feat/testing/b025_000965.npz\n",
      "Batch:  429\n",
      "feature_path:: data/dad/obj_feat/testing/b036_001093.npz\n",
      "Batch:  430\n",
      "feature_path:: data/dad/obj_feat/testing/b020_000549.npz\n",
      "Batch:  431\n",
      "feature_path:: data/dad/obj_feat/testing/b022_000543.npz\n",
      "Batch:  432\n",
      "feature_path:: data/dad/obj_feat/testing/b046_000521.npz\n",
      "Batch:  433\n",
      "feature_path:: data/dad/obj_feat/testing/b029_001025.npz\n",
      "Batch:  434\n",
      "feature_path:: data/dad/obj_feat/testing/b001_000888.npz\n",
      "Batch:  435\n",
      "feature_path:: data/dad/obj_feat/testing/b009_000869.npz\n",
      "Batch:  436\n",
      "feature_path:: data/dad/obj_feat/testing/b014_000922.npz\n",
      "Batch:  437\n",
      "feature_path:: data/dad/obj_feat/testing/b002_000891.npz\n",
      "Batch:  438\n",
      "feature_path:: data/dad/obj_feat/testing/b034_001102.npz\n",
      "Batch:  439\n",
      "feature_path:: data/dad/obj_feat/testing/b014_000839.npz\n",
      "Batch:  440\n",
      "feature_path:: data/dad/obj_feat/testing/b016_000996.npz\n",
      "Batch:  441\n",
      "feature_path:: data/dad/obj_feat/testing/b044_000573.npz\n",
      "Batch:  442\n",
      "feature_path:: data/dad/obj_feat/testing/b040_001075.npz\n",
      "Batch:  443\n",
      "Batch:  444\n",
      "Batch:  445\n",
      "Batch:  446\n",
      "Batch:  447\n",
      "Batch:  448\n",
      "Batch:  449\n",
      "Batch:  450\n",
      "Batch:  451\n",
      "Batch:  452\n",
      "Batch:  453\n",
      "Batch:  454\n",
      "Batch:  455\n",
      "Batch:  456\n",
      "Batch:  457\n",
      "Batch:  458\n",
      "Batch:  459\n",
      "All toa : [101, 90, 90, 90, 101, 101, 101, 101, 101, 101, 90, 101, 101, 101, 101, 101, 101, 90, 101, 101, 101, 101, 90, 101, 101, 101, 101, 90, 90, 90, 101, 101, 101, 101, 90, 101, 90, 101, 101, 101, 90, 101, 90, 90, 90, 101, 90, 101, 101, 101, 101, 90, 101, 101, 101, 101, 101, 90, 101, 90, 101, 101, 101, 101, 101, 90, 101, 101, 90, 101, 90, 101, 101, 101, 90, 90, 101, 90, 101, 101, 101, 101, 90, 90, 101, 101, 90, 101, 101, 90, 101, 101, 101, 101, 90, 101, 90, 90, 90, 101, 90, 101, 101, 90, 90, 101, 101, 101, 90, 90, 90, 90, 90, 101, 90, 90, 90, 101, 101, 90, 90, 101, 101, 101, 101, 101, 90, 90, 90, 90, 90, 90, 101, 101, 101, 101, 90, 101, 101, 101, 101, 101, 90, 101, 101, 90, 90, 101, 90, 90, 101, 90, 90, 101, 90, 90, 101, 101, 101, 90, 101, 101, 101, 101, 101, 101, 101, 101, 90, 90, 90, 101, 101, 101, 90, 101, 90, 101, 101, 90, 90, 90, 101, 101, 101, 90, 101, 101, 101, 101, 101, 101, 90, 101, 90, 101, 101, 101, 90, 90, 101, 90, 101, 101, 101, 101, 90, 101, 101, 101, 101, 101, 101, 101, 101, 90, 101, 101, 101, 90, 101, 101, 101, 101, 101, 101, 101, 90, 90, 90, 101, 90, 101, 101, 101, 90, 90, 90, 101, 101, 90, 101, 101, 90, 101, 90, 90, 90, 90, 90, 101, 101, 101, 101, 101, 101, 90, 90, 90, 101, 90, 101, 90, 101, 101, 90, 90, 101, 101, 101, 101, 90, 101, 101, 101, 90, 101, 90, 101, 101, 101, 101, 101, 101, 90, 101, 101, 101, 90, 101, 101, 101, 90, 90, 101, 101, 101, 90, 90, 90, 101, 101, 101, 101, 90, 90, 101, 101, 90, 90, 101, 101, 101, 101, 101, 101, 90, 101, 90, 101, 90, 90, 90, 101, 101, 101, 101, 101, 101, 90, 90, 90, 101, 101, 101, 101, 101, 101, 90, 101, 90, 101, 101, 90, 101, 90, 101, 90, 101, 90, 101, 90, 101, 101, 101, 90, 101, 90, 101, 90, 101, 101, 101, 90, 101, 90, 101, 101, 101, 101, 101, 101, 101, 101, 90, 90, 90, 101, 101, 90, 90, 101, 101, 90, 101, 90, 90, 101, 101, 90, 101, 101, 101, 101, 101, 90, 101, 101, 90, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 90, 101, 90, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 90, 90, 101, 101, 90, 101, 90, 101, 101, 101, 101, 101, 90, 101, 101, 90, 101, 101, 90, 101, 101, 101, 101, 90, 90, 90, 101, 101, 101, 101, 101, 101, 101, 101, 90, 101]\n",
      "All Probs vid2 :  460\n",
      "All y vid :  460\n"
     ]
    }
   ],
   "source": [
    "test_only =1\n",
    "if bool(test_only):\n",
    "    all_probs_vid2, all_y_vid, all_toa  = test_model(0, model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66976645",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[25], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_accidents = all_toa\n",
    "all_labels = all_y_vid.numpy()\n",
    "all_pred = all_probs_vid2.numpy()\n",
    "fps=20.0\n",
    "preds_eval = []\n",
    "min_pred = np.inf\n",
    "n_frames = 0\n",
    "for idx, toa in enumerate(time_of_accidents):\n",
    "    print(\"Idx %d, toa %d\"%(idx, toa))\n",
    "    if all_labels[idx] > 0:\n",
    "        pred = all_pred[idx, :int(toa)]  # positive video\n",
    "    else:\n",
    "        pred = all_pred[idx, :]  # negative video\n",
    "    # find the minimum prediction\n",
    "    min_pred = np.min(pred) if min_pred > np.min(pred) else min_pred\n",
    "    preds_eval.append(pred)\n",
    "    n_frames += len(pred)\n",
    "total_seconds = all_pred.shape[1] / fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate a set of thresholds from the minimum predictions\n",
    "# temp_shape = int((1.0 - max(min_pred, 0)) / 0.001 + 0.5) \n",
    "print(\"n_frames : \",n_frames)\n",
    "n_frames = 741\n",
    "Precision = np.zeros((n_frames))\n",
    "Recall = np.zeros((n_frames))\n",
    "Time = np.zeros((n_frames))\n",
    "cnt = 0\n",
    "print(\"N_frames: \", n_frames)\n",
    "print(\"Precision: \", Precision.shape)\n",
    "print(\"Min Pred : \",min_pred)\n",
    "print(np.arange(max(min_pred, 0), 1.0, 0.001).shape)\n",
    "print(\"Len Pred Eval: \", preds_eval[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Th in np.arange(max(min_pred, 0), 1.0, 0.001):\n",
    "    print(\"TH:\",Th)\n",
    "    Tp = 0.0\n",
    "    Tp_Fp = 0.0\n",
    "    Tp_Tn = 0.0\n",
    "    time = 0.0\n",
    "    counter = 0.0  # number of TP videos\n",
    "    # iterate each video sample\n",
    "    for i in range(len(preds_eval)):\n",
    "        # true positive frames: (pred->1) * (gt->1)\n",
    "        tp =  np.where(preds_eval[i]*all_labels[i]>=Th)\n",
    "        Tp += float(len(tp[0])>0)\n",
    "        if float(len(tp[0])>0) > 0:\n",
    "            # if at least one TP, compute the relative (1 - rTTA)\n",
    "            time += tp[0][0] / float(time_of_accidents[i])\n",
    "            counter = counter+1\n",
    "        # all positive frames\n",
    "        Tp_Fp += float(len(np.where(preds_eval[i]>=Th)[0])>0)\n",
    "    \n",
    "\n",
    "    print(\"TPFP : \",Tp_Fp)\n",
    "    \n",
    "    if Tp_Fp == 0:  # predictions of all videos are negative\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            Precision[cnt] = Tp/Tp_Fp\n",
    "        except ValueError as e:\n",
    "            print(\"Error : \",e)\n",
    "    if np.sum(all_labels) ==0: # gt of all videos are negative\n",
    "        continue\n",
    "    else:\n",
    "        Recall[cnt] = Tp/np.sum(all_labels)\n",
    "    if counter == 0:\n",
    "        continue\n",
    "    else:\n",
    "        Time[cnt] = (1-time/counter)\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0982dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = np.argsort(Recall)\n",
    "Precision = Precision[new_index]\n",
    "Recall = Recall[new_index]\n",
    "Time = Time[new_index]\n",
    "print(\"New index: \", new_index.shape)\n",
    "print(\"Precision: \", Precision[new_index].shape)\n",
    "print(\"Recall: \", Recall[new_index].shape)\n",
    "print(\"Time: \", Time[new_index].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unique the recall, and fetch corresponding precisions and TTAs\n",
    "_,rep_index = np.unique(Recall,return_index=1)\n",
    "rep_index = rep_index[1:]\n",
    "new_Time = np.zeros(len(rep_index))\n",
    "new_Precision = np.zeros(len(rep_index))\n",
    "for i in range(len(rep_index)-1):\n",
    "        new_Time[i] = np.max(Time[rep_index[i]:rep_index[i+1]])\n",
    "        new_Precision[i] = np.max(Precision[rep_index[i]:rep_index[i+1]])\n",
    "# sort by descending order\n",
    "new_Time[-1] = Time[rep_index[-1]]\n",
    "new_Precision[-1] = Precision[rep_index[-1]]\n",
    "new_Recall = Recall[rep_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d583c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AP = 0.0\n",
    "if new_Recall[0] != 0:\n",
    "    AP += new_Precision[0]*(new_Recall[0]-0)\n",
    "for i in range(1,len(new_Precision)):\n",
    "    AP += (new_Precision[i-1]+new_Precision[i])*(new_Recall[i]-new_Recall[i-1])/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the relative mTTA to seconds\n",
    "mTTA = np.mean(new_Time) * total_seconds\n",
    "print(\"Average Precision= %.4f, mean Time to accident= %.4f\"%(AP, mTTA))\n",
    "sort_time = new_Time[np.argsort(new_Recall)]\n",
    "sort_recall = np.sort(new_Recall)\n",
    "TTA_R80 = sort_time[np.argmin(np.abs(sort_recall-0.8))] * total_seconds\n",
    "print(\"Recall@80%, Time to accident= \" +\"{:.4}\".format(TTA_R80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095fde4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33074c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def video_to_npy(video_path, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert frame to RGB (OpenCV uses BGR by default)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Convert list to NumPy array\n",
    "    frames_array = np.array(frames)\n",
    "    \n",
    "    # Save to .npy file\n",
    "    np.save(output_path, frames_array)\n",
    "    print(f\"Saved frames to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "video_path = '/Users/sacithrangana/Desktop/Msc/AAI/text.mp4'\n",
    "output_npy_path = './output_frames.npy'\n",
    "video_to_npy(video_path, output_npy_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0cb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6d8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def split_video_to_clips(video_path, output_dir, clip_duration=5, fps=20):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Calculate the number of frames per clip\n",
    "    clip_frames = int(clip_duration * fps)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    current_clip = 0\n",
    "    current_frame = 0\n",
    "    \n",
    "    while True:\n",
    "        frames = []\n",
    "        for _ in range(clip_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "            current_frame += 1\n",
    "        \n",
    "        if not frames:\n",
    "            break\n",
    "        \n",
    "        # Save current clip\n",
    "        clip_filename = os.path.join(output_dir, f\"clip_{current_clip:04d}.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(clip_filename, fourcc, fps, (width, height))\n",
    "        \n",
    "        for frame in frames:\n",
    "            out.write(frame)\n",
    "        \n",
    "        out.release()\n",
    "        print(f\"Saved clip: {clip_filename}\")\n",
    "        \n",
    "        current_clip += 1\n",
    "        \n",
    "        if current_frame >= total_frames:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "# Example usage\n",
    "video_path = '/Users/sacithrangana/Desktop/Msc/AAI/text.mp4'\n",
    "output_dir = './input_clips/positive'\n",
    "split_video_to_clips(video_path, output_dir)\n",
    "\n",
    "video_path = '/Users/sacithrangana/Downloads/video2.mp4'\n",
    "output_dir = './input_clips/negative'\n",
    "split_video_to_clips(video_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852bde02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def video_to_npy(video_path, output_path):\n",
    "#     # Open the video file\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "#     frames = []\n",
    "    \n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         # Convert frame to RGB (OpenCV uses BGR by default)\n",
    "#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         frames.append(frame)\n",
    "    \n",
    "#     cap.release()\n",
    "    \n",
    "#     # Convert list to NumPy array\n",
    "#     frames_array = np.array(frames)\n",
    "    \n",
    "#     # print(output_path)\n",
    "#     # Save to .npy file\n",
    "#     # np.save(output_path, frames_array)\n",
    "#     print(f\"Saved frames to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "# video_path_positive = './input_clips/positive'\n",
    "# video_path_negative = './input_clips/negative'\n",
    "\n",
    "\n",
    "# # loop through all the clips in the output directory\n",
    "# for clip in os.listdir(video_path_positive):\n",
    "#     output_npy_path = './output_frames/positive/'\n",
    "#     clip_path = os.path.join(output_dir, clip)\n",
    "#     clip = clip.split(\".\")[0]\n",
    "#     output_npy_path = os.path.join(output_npy_path, f\"{clip}\"   + \".npy\")\n",
    "#     video_to_npy(video_path_positive, output_npy_path)\n",
    "\n",
    "# for clip in os.listdir(video_path_negative):\n",
    "#     output_npy_path = './output_frames/nagative/'\n",
    "#     clip_path = os.path.join(output_dir, clip)\n",
    "#     clip = clip.split(\".\")[0]\n",
    "#     output_npy_path = os.path.join(output_npy_path, f\"{clip}\"   + \".npy\")\n",
    "#     video_to_npy(video_path_negative, output_npy_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927bfde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072eecf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded499ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def video_to_3d_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert frame to grayscale (2D)\n",
    "        # gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def process_videos(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        frames = video_to_3d_frames(video_path)\n",
    "        output_path = os.path.join(output_folder, os.path.splitext(video_file)[0] + '.npy')\n",
    "        np.save(output_path, frames)\n",
    "        print(f\"Saved {output_path}\")\n",
    "\n",
    "def plot_frames_from_npy_sorted(output_folder, num_frames=9):\n",
    "    npy_files = [f for f in os.listdir(output_folder) if f.endswith('.npy')]\n",
    "    \n",
    "    # Sort files based on clip number assuming filenames are in format 'clip_<number>.npy'\n",
    "    npy_files.sort(key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for i in range(min(num_frames, len(npy_files))):\n",
    "        npy_path = os.path.join(output_folder, npy_files[i])\n",
    "        frames = np.load(npy_path)\n",
    "        \n",
    "        # Plot the first frame of each video\n",
    "        if frames.size > 0:\n",
    "            plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(frames[0], cmap='gray')\n",
    "            plt.title(os.path.basename(npy_path))\n",
    "            plt.axis('off')\n",
    "    plt.savefig('2d_frames.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'input_clips/positive'\n",
    "output_folder = 'output_frames/positve'\n",
    "process_videos(input_folder, output_folder)\n",
    "plot_frames_from_npy_sorted(output_folder)\n",
    "\n",
    "input_folder = 'input_clips/negative'\n",
    "output_folder = 'output_frames/negative'\n",
    "process_videos(input_folder, output_folder)\n",
    "plot_frames_from_npy_sorted(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def video_to_3d_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Append the frame as it is (3D: height, width, channels)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def process_videos_to_individual_npz(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        frames = video_to_3d_frames(video_path)\n",
    "        video_key = os.path.splitext(video_file)[0]\n",
    "        \n",
    "        # Save each video's frames into a separate .npz file\n",
    "        output_path = os.path.join(output_folder, f'{video_key}.npz')\n",
    "        np.savez(output_path, frames=frames)\n",
    "        print(f\"Saved {output_path}\")\n",
    "\n",
    "def plot_frames_from_npz_sorted(output_folder, num_videos=9):\n",
    "    npz_files = [f for f in os.listdir(output_folder) if f.endswith('.npz')]\n",
    "    \n",
    "    # Sort files based on clip number assuming filenames are in format 'clip_<number>.npz'\n",
    "    npz_files.sort(key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for i in range(min(num_videos, len(npz_files))):\n",
    "        npz_path = os.path.join(output_folder, npz_files[i])\n",
    "        data = np.load(npz_path)\n",
    "        frames = data['frames']\n",
    "        \n",
    "        # Plot the first frame of each video\n",
    "        if frames.size > 0:\n",
    "            plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(cv2.cvtColor(frames[0], cv2.COLOR_BGR2RGB))\n",
    "            plt.title(os.path.splitext(npz_files[i])[0])\n",
    "            plt.axis('off')\n",
    "    \n",
    "    # plt.savefig('3d_frames.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'input_clips/positive'\n",
    "output_folder = 'output_npz/positive'\n",
    "process_videos_to_individual_npz(input_folder, output_folder)\n",
    "\n",
    "# Plotting frames from the .npz files in sorted order\n",
    "plot_frames_from_npz_sorted(output_folder)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'input_clips/negative'\n",
    "output_folder = 'output_npz/negative'\n",
    "process_videos_to_individual_npz(input_folder, output_folder)\n",
    "\n",
    "# Plotting frames from the .npz files in sorted order\n",
    "plot_frames_from_npz_sorted(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and configurations\n",
    "video_path = './input_clips/positive/clip_0000.mp4'\n",
    "vgg16_model = init_vgg16()\n",
    "det_model = init_mmdet_model('path/to/config_file.py', 'path/to/checkpoint_file.pth')\n",
    "\n",
    "# Process video\n",
    "frame_features, box_features, bboxes = process_video(video_path, vgg16_model, det_model)\n",
    "\n",
    "# Define labels and ID\n",
    "labels = np.array([0, 1])  # Example labels\n",
    "video_id = 'video_001'\n",
    "\n",
    "# Save features to .npz\n",
    "save_npz('path/to/output_file.npz', frame_features, box_features, bboxes, labels, video_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeef45d",
   "metadata": {},
   "source": [
    "Frame Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae89c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(11, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(54, 2)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def main(video_path, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Video properties\n",
    "    fps = 20\n",
    "    duration = 5\n",
    "    total_frames = fps * duration\n",
    "\n",
    "    # Initialize list to store statistics\n",
    "    frame_stats = []\n",
    "\n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        # Append statistics to the list\n",
    "        frame_stats.append([frame_width, frame_height])\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Convert the list to a 2D numpy array\n",
    "    frame_stats_array = np.array(frame_stats)\n",
    "\n",
    "    print(frame_stats_array.shape)  # Should print (100, 2)\n",
    "    np.save(output_path, frame_stats_array)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    input_folder = 'input_clips/negative'\n",
    "    output_folder = 'data/test/frames_stats/testing/negative'\n",
    "    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        output_path = os.path.join(output_folder, os.path.splitext(video_file)[0][5:] + '.npy')\n",
    "        main(video_path, output_path)\n",
    "\n",
    "    input_folder = 'input_clips/positive'\n",
    "    output_folder = 'data/test/frames_stats/testing/positive'\n",
    "    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        output_path = os.path.join(output_folder, os.path.splitext(video_file)[0][5:] + '.npy')\n",
    "        main(video_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e4d59",
   "metadata": {},
   "source": [
    "VG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75e806b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class VGG16FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16FeatureExtractor, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = vgg16.features\n",
    "        self.avgpool = vgg16.avgpool\n",
    "        self.classifier = nn.Sequential(*list(vgg16.classifier.children())[:-1])  # remove the last layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def extract_features_from_video(video_path, extractor, transform, frame_count=50, box_count=19):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < frame_count:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        frame = transform(frame)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    frames = torch.stack(frames)\n",
    "    frame_features = extractor(frames)\n",
    "    frame_level_features = frame_features.unsqueeze(1)  # Add dimension for concatenation\n",
    "\n",
    "    box_level_features = torch.randn(frame_count, box_count, 4096)  # Dummy data for box-level features\n",
    "    bounding_boxes = torch.randn(frame_count, box_count, 6)  # Dummy data for bounding boxes\n",
    "\n",
    "    data = torch.cat([frame_level_features, box_level_features], dim=1)\n",
    "    return data, bounding_boxes\n",
    "\n",
    "def main(output_path, positive, frames, video_path):\n",
    "    model = VGG16FeatureExtractor()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    data, det = extract_features_from_video(video_path, model, transform, frames)\n",
    "    if positive == 1:\n",
    "        labels = np.array([0, 1])  # Example label, adjust as needed\n",
    "    else:\n",
    "        labels = np.array([1, 0]) \n",
    "    video_id = os.path.basename(video_path)\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    data_np = data.detach().numpy()\n",
    "    det_np = det.detach().numpy()\n",
    "\n",
    "    # Save data into .npz file\n",
    "    np.savez(output_path, data=data_np, det=det_np, labels=labels, ID=video_id)\n",
    "\n",
    "    print(f'Data shape: {data_np.shape}')  # Should be (50, 20, 4096)\n",
    "    print(f'Detected bounding boxes shape: {det_np.shape}')  # Should be (50, 19, 6)\n",
    "    print(f'Labels: {labels}')\n",
    "    print(f'Video ID: {video_id}')\n",
    "    print(f'Saved data to {output_path}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # def process_videos(input_folder, output_folder):\n",
    "    # if not os.path.exists(output_folder):\n",
    "    #     os.makedirs(output_folder)\n",
    "    input_folder = 'input_clips/negative'\n",
    "    output_folder = 'data/test/obj_feat/testing'\n",
    "    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
    "    \n",
    "    # for video_file in video_files:\n",
    "    #     video_path = os.path.join(input_folder, video_file)\n",
    "    #     print(video_path)\n",
    "    #     output_path = os.path.join(output_folder, os.path.splitext(video_file)[0][5:] + '.npz')\n",
    "    #     main(output_path, 0, 100, video_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25cf3e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (11, 20, 4096)\n",
      "Detected bounding boxes shape: (11, 19, 6)\n",
      "Labels: [1 0]\n",
      "Video ID: clip_0006.mp4\n",
      "Saved data to /Users/sacithrangana/Desktop/Msc/AAI/gitrepo/Graph_Code/Graph-Graph/data/test/obj_feat/testing\n"
     ]
    }
   ],
   "source": [
    "main('/Users/sacithrangana/Desktop/Msc/AAI/gitrepo/Graph_Code/Graph-Graph/data/test/obj_feat/testing', 0, 11, '/Users/sacithrangana/Desktop/Msc/AAI/gitrepo/Graph_Code/Graph-Graph/input_clips/negative/clip_0006.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa7905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (100, 20, 4096)\n",
      "Detected bounding boxes shape: (100, 19, 6)\n",
      "Labels: [0 1]\n",
      "Video ID: clip_0016.mp4\n",
      "Data shape: (100, 20, 4096)\n",
      "Detected bounding boxes shape: (100, 19, 6)\n",
      "Labels: [0 1]\n",
      "Video ID: clip_0015.mp4\n",
      "Data shape: (100, 20, 4096)\n",
      "Detected bounding boxes shape: (100, 19, 6)\n",
      "Labels: [0 1]\n",
      "Video ID: clip_0014.mp4\n",
      "Data shape: (100, 20, 4096)\n",
      "Detected bounding boxes shape: (100, 19, 6)\n",
      "Labels: [0 1]\n",
      "Video ID: clip_0010.mp4\n",
      "Data shape: (100, 20, 4096)\n",
      "Detected bounding boxes shape: (100, 19, 6)\n",
      "Labels: [0 1]\n",
      "Video ID: clip_0011.mp4\n",
      "Data shape: (100, 20, 4096)\n",
      "Detected bounding boxes shape: (100, 19, 6)\n",
      "Labels: [0 1]\n",
      "Video ID: clip_0013.mp4\n",
      "Data shape: (100, 20, 4096)\n",
      "Detected bounding boxes shape: (100, 19, 6)\n",
      "Labels: [0 1]\n",
      "Video ID: clip_0007.mp4\n",
      "Data shape: (100, 20, 4096)\n",
      "Detected bounding boxes shape: (100, 19, 6)\n",
      "Labels: [0 1]\n",
      "Video ID: clip_0012.mp4\n",
      "Data shape: (100, 20, 4096)\n",
      "Detected bounding boxes shape: (100, 19, 6)\n",
      "Labels: [0 1]\n",
      "Video ID: clip_0008.mp4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 54 but got size 100 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, video_file)\n\u001b[1;32m      7\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(video_file)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m5\u001b[39m:]\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 55\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(output_path, positive, frames, video_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m model \u001b[38;5;241m=\u001b[39m VGG16FeatureExtractor()\n\u001b[1;32m     49\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     50\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToPILImage(),\n\u001b[1;32m     51\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     52\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[1;32m     53\u001b[0m ])\n\u001b[0;32m---> 55\u001b[0m data, det \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m positive \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     57\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Example label, adjust as needed\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mextract_features_from_video\u001b[0;34m(video_path, extractor, transform, frame_count, box_count)\u001b[0m\n\u001b[1;32m     41\u001b[0m box_level_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(frame_count, box_count, \u001b[38;5;241m4096\u001b[39m)  \u001b[38;5;66;03m# Dummy data for box-level features\u001b[39;00m\n\u001b[1;32m     42\u001b[0m bounding_boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(frame_count, box_count, \u001b[38;5;241m6\u001b[39m)  \u001b[38;5;66;03m# Dummy data for bounding boxes\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframe_level_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_level_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, bounding_boxes\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 54 but got size 100 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "input_folder = 'input_clips/positive'\n",
    "output_folder = 'data/test/obj_feat/testing'\n",
    "video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
    "    \n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(input_folder, video_file)\n",
    "    output_path = os.path.join(output_folder, os.path.splitext(video_file)[0][5:]+ '.npz')\n",
    "    main(output_path, 1, 100, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c593151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (54, 20, 4096)\n",
      "Detected bounding boxes shape: (54, 19, 6)\n",
      "Labels: [0 1]\n",
      "Video ID: clip_0009.mp4\n"
     ]
    }
   ],
   "source": [
    "main('/Users/sacithrangana/Desktop/Msc/AAI/gitrepo/Graph_Code/Graph-Graph/data/test/obj_feat/testing', 1,54, '/Users/sacithrangana/Desktop/Msc/AAI/gitrepo/Graph_Code/Graph-Graph/input_clips/positive/clip_0009.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734672a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a381c6fa",
   "metadata": {},
   "source": [
    "I3D feartures with VG16 - tuvan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b20d6e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 22:13:17.389778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def main_vgg(video_path, output_path):\n",
    "    # Load the VGG16 model pre-trained on ImageNet and remove the top layer\n",
    "    base_model = VGG16(weights='imagenet', include_top=False)\n",
    "    model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "    print(\"loaded\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print(\"video loaded\")       \n",
    "    # Define the number of frames and the embedding size\n",
    "    num_frames = 100\n",
    "    embedding_size = 2048  # This size is the output shape after the VGG16 layer\n",
    "\n",
    "    # Create an array to hold the embeddings\n",
    "    embeddings = np.zeros((num_frames, embedding_size))\n",
    "\n",
    "    frame_count = 0\n",
    "    while frame_count < num_frames:\n",
    "        print(\"frame:: \", frame_count)   \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess the frame\n",
    "        frame = cv2.resize(frame, (224, 224))  # Resize to VGG16 expected input size\n",
    "        frame = frame.astype('float32')\n",
    "        frame = preprocess_input(frame)\n",
    "        frame = np.expand_dims(frame, axis=0)\n",
    "\n",
    "        # Extract features using VGG16\n",
    "        features = model.predict(frame)\n",
    "        embeddings[frame_count] = np.squeeze(features).flatten()[:embedding_size]\n",
    "        \n",
    "        \n",
    "        frame_count += 1\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Save the embeddings as a .npy file\n",
    "    np.save(output_path, embeddings)\n",
    "    print(f\"Saved embeddings to {output_npy_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b0e005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "video loaded\n",
      "frame::  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_folder = 'input_clips/positive'\n",
    "output_folder = 'data/test/i3d_feat/testing/positive'\n",
    "video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
    "    \n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(input_folder, video_file)\n",
    "    output_path = os.path.join(output_folder, os.path.splitext(video_file)[0][5:]+ '.npy')\n",
    "    main_vgg(video_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the VGG16 model pre-trained on ImageNet and remove the top layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Initialize video capture\n",
    "video_path = 'clip_1.mp4'  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Define the number of frames and the embedding size\n",
    "num_frames = 100\n",
    "embedding_size = 2048  # This size is the output shape after the VGG16 layer\n",
    "\n",
    "# Create an array to hold the embeddings\n",
    "embeddings = np.zeros((num_frames, embedding_size))\n",
    "\n",
    "frame_count = 0\n",
    "while frame_count < num_frames:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Preprocess the frame\n",
    "    frame = cv2.resize(frame, (224, 224))  # Resize to VGG16 expected input size\n",
    "    frame = frame.astype('float32')\n",
    "    frame = preprocess_input(frame)\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "\n",
    "    # Extract features using VGG16\n",
    "    features = model.predict(frame)\n",
    "    embeddings[frame_count] = np.squeeze(features).flatten()[:embedding_size]\n",
    "    \n",
    "    \n",
    "    frame_count += 1\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Save the embeddings as a .npy file\n",
    "np.save('embeddings.npy', embeddings)\n",
    "\n",
    "print(\"Embeddings saved to 'embeddings.npy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc986f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6439d2f7",
   "metadata": {},
   "source": [
    "I3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def main(output_path):\n",
    "    # Initialize VGG16 model\n",
    "    model = models.vgg16(pretrained=True).features\n",
    "    model.eval()\n",
    "\n",
    "    # Define preprocessing transform\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Load video\n",
    "    video_path = './input_clips/negative/clip_0000.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Collect frames\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Process frames\n",
    "    processed_frames = [preprocess(frame) for frame in frames]\n",
    "    frame_tensor = torch.stack(processed_frames)\n",
    "\n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = model(frame_tensor).view(len(frames), -1)\n",
    "\n",
    "    # Limit features to (100, 2048)\n",
    "    if features.shape[0] > 100:\n",
    "        features = features[:100]\n",
    "    elif features.shape[0] < 100:\n",
    "        # Handle case where there are fewer than 100 frames\n",
    "        padding = torch.zeros(100 - features.shape[0], features.shape[1])\n",
    "        features = torch.cat([features, padding])\n",
    "\n",
    "    # Resize feature vector to (2048)\n",
    "    features = features[:, :2048]\n",
    "\n",
    "    # Convert features to numpy array\n",
    "    features_np = features.numpy()\n",
    "\n",
    "    np.save(output_path, features_np)\n",
    "\n",
    "    print(\"Features shape:\", features_np.shape)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    input_folder = 'input_clips/positive'\n",
    "    output_folder = 'data/test/i3d_feat/testing/positive'\n",
    "    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
    "        \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        output_path = os.path.join(output_folder, os.path.splitext(video_file)[0]+ '.npy')\n",
    "        main(output_path)\n",
    "\n",
    "    input_folder = 'input_clips/negative'\n",
    "    output_folder = 'data/test/i3d_feat/testing/negative'\n",
    "    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
    "        \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        output_path = os.path.join(output_folder, os.path.splitext(video_file)[0] + '.npy')\n",
    "        main(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0db9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b37158b1",
   "metadata": {},
   "source": [
    "VG16 New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load pre-trained VGG-16 model\n",
    "# vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "# vgg16.eval()\n",
    "\n",
    "# # Modify VGG-16 to use as a feature extractor\n",
    "# feature_extractor = torch.nn.Sequential(*list(vgg16.features.children()))\n",
    "\n",
    "# # Load a pre-trained Faster R-CNN model for object detection\n",
    "# faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# faster_rcnn.eval()\n",
    "\n",
    "\n",
    "# def extract_frames(video_path):\n",
    "#     vidcap = cv2.VideoCapture(video_path)\n",
    "#     frames = []\n",
    "#     success, image = vidcap.read()\n",
    "#     while success:\n",
    "#         frames.append(image)\n",
    "#         success, image = vidcap.read()\n",
    "#     vidcap.release()\n",
    "#     return frames\n",
    "\n",
    "# video_path = './input_clips/positive/clip_0007.mp4'\n",
    "# frames = extract_frames(video_path)\n",
    "\n",
    "\n",
    "# def preprocess_frame(frame):\n",
    "#     # Convert BGR (OpenCV) to RGB\n",
    "#     frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     # Resize to 224x224 as expected by VGG-16\n",
    "#     frame_resized = cv2.resize(frame_rgb, (224, 224))\n",
    "#     # Convert to tensor and normalize\n",
    "#     frame_tensor = torchvision.transforms.functional.to_tensor(frame_resized)\n",
    "#     frame_tensor = torchvision.transforms.functional.normalize(frame_tensor, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     return frame_tensor\n",
    "\n",
    "# preprocessed_frames = [preprocess_frame(frame) for frame in frames]\n",
    "\n",
    "\n",
    "\n",
    "# # Convert list of frames to batch tensor\n",
    "# batch_tensor = torch.stack(preprocessed_frames)\n",
    "\n",
    "# # Extract features using VGG-16\n",
    "# with torch.no_grad():\n",
    "#     features = feature_extractor(batch_tensor)\n",
    "\n",
    "# # Perform object detection using Faster R-CNN\n",
    "# with torch.no_grad():\n",
    "#     detections = faster_rcnn(batch_tensor)\n",
    "\n",
    "# # Display detections (for the first frame as an example)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def display_detections(image, detections):\n",
    "#     for box, score, label in zip(detections['boxes'], detections['scores'], detections['labels']):\n",
    "#         if score > 0.5:  # Adjust threshold as needed\n",
    "#             x1, y1, x2, y2 = box\n",
    "#             cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "\n",
    "# # Display detections for the first frame\n",
    "# display_detections(frames[0], detections[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load a pre-trained object detection model\n",
    "detection_model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "detection_model.eval()\n",
    "\n",
    "# Define transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_features_and_draw_boxes(video_path, positivity=True, frame_count=100):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_features = []\n",
    "    bounding_boxes = []  # To store bounding boxes (x1, y1, x2, y2, prob, cls)\n",
    "    one_hot_labels = []  # To store one-hot labels\n",
    "\n",
    "    for _ in range(frame_count):\n",
    "        print(_)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Object detection\n",
    "        input_tensor = preprocess(frame).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            detection = detection_model([input_tensor.squeeze()])[0]\n",
    "        \n",
    "        # Draw bounding boxes on the frame\n",
    "        boxes = detection['boxes'].cpu().numpy()\n",
    "        scores = detection['scores'].cpu().numpy()\n",
    "        labels = detection['labels'].cpu().numpy()\n",
    "        \n",
    "        # Filter boxes with a score threshold (e.g., 0.5)\n",
    "        indices = np.where(scores > 0.5)[0]\n",
    "        filtered_boxes = boxes[indices]\n",
    "        filtered_scores = scores[indices]\n",
    "        filtered_labels = labels[indices]\n",
    "        \n",
    "        for i in range(len(filtered_boxes)):\n",
    "            box = filtered_boxes[i]\n",
    "            score = filtered_scores[i]\n",
    "            label = filtered_labels[i]\n",
    "            cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f'{label}: {score:.2f}', (int(box[0]), int(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        \n",
    "        # Save the frame with drawn bounding boxes\n",
    "        cv2.imwrite(f'frame_with_boxes_{_}.jpg', frame)\n",
    "        \n",
    "        # Frame-level feature extraction\n",
    "        frame_input_tensor = preprocess(frame).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            frame_feature = vgg16(frame_input_tensor).numpy().flatten()\n",
    "        frame_features.append(frame_feature)\n",
    "\n",
    "        # Store bounding boxes and labels\n",
    "        boxes_with_scores_labels = np.hstack([filtered_boxes, filtered_scores[:, np.newaxis], filtered_labels[:, np.newaxis]])\n",
    "        bounding_boxes.append(boxes_with_scores_labels)\n",
    "\n",
    "        label = [0, 1] if positivity else [1, 0]\n",
    "        one_hot_labels.append(label)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    frame_features = np.array(frame_features)\n",
    "    bounding_boxes = np.array(bounding_boxes)\n",
    "    one_hot_labels = np.array(one_hot_labels)\n",
    "\n",
    "    # Shape the features\n",
    "    frame_level_features = frame_features.reshape(-1, 1, 4096)\n",
    "    box_level_features = np.tile(frame_features, (1, 19)).reshape(-1, 19, 4096)\n",
    "    data = np.concatenate((frame_level_features, box_level_features), axis=1)\n",
    "\n",
    "    return {\n",
    "        'data': data,\n",
    "        'det': bounding_boxes,\n",
    "        'labels': one_hot_labels,\n",
    "        'ID': video_path.split('/')[-1]\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "video_path = './input_clips/positive/clip_0007.mp4'\n",
    "features = extract_features_and_draw_boxes(video_path)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48713f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained VGG-16 model\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "vgg16.eval()\n",
    "\n",
    "# Modify VGG-16 to use as a feature extractor\n",
    "feature_extractor = torch.nn.Sequential(*list(vgg16.features.children()))\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model for object detection\n",
    "faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "faster_rcnn.eval()\n",
    "\n",
    "\n",
    "def extract_frames(video_path):\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    success, image = vidcap.read()\n",
    "    while success:\n",
    "        frames.append(image)\n",
    "        success, image = vidcap.read()\n",
    "    vidcap.release()\n",
    "    return frames\n",
    "\n",
    "video_path = './input_clips/positive/clip_0007.mp4'\n",
    "frames = extract_frames(video_path)\n",
    "\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    # Convert BGR (OpenCV) to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Resize to 224x224 as expected by VGG-16\n",
    "    frame_resized = cv2.resize(frame_rgb, (224, 224))\n",
    "    # Convert to tensor and normalize\n",
    "    frame_tensor = torchvision.transforms.functional.to_tensor(frame_resized)\n",
    "    frame_tensor = torchvision.transforms.functional.normalize(frame_tensor, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    return frame_tensor\n",
    "\n",
    "preprocessed_frames = [preprocess_frame(frame) for frame in frames]\n",
    "\n",
    "\n",
    "\n",
    "# Convert list of frames to batch tensor\n",
    "batch_tensor = torch.stack(preprocessed_frames)\n",
    "\n",
    "# Extract features using VGG-16\n",
    "with torch.no_grad():\n",
    "    features = feature_extractor(batch_tensor)\n",
    "\n",
    "# Perform object detection using Faster R-CNN\n",
    "with torch.no_grad():\n",
    "    detections = faster_rcnn(batch_tensor)\n",
    "\n",
    "# Display detections (for the first frame as an example)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_detections(image, detections):\n",
    "    for box, score, label in zip(detections['boxes'], detections['scores'], detections['labels']):\n",
    "        if score > 0.5:  # Adjust threshold as needed\n",
    "            x1, y1, x2, y2 = box\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Display detections for the first frame\n",
    "display_detections(frames[0], detections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fecfddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load Video\n",
    "cap = cv2.VideoCapture(\"./input_clips/positive/clip_0007.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Detecting objects\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Showing information on the screen\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506bb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c42e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6620e80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d202d981",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 123\u001b[0m\n\u001b[1;32m    119\u001b[0m input_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_clips/positive\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_npz/positive\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 123\u001b[0m \u001b[43minit_vgg16\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./input_clips/positive/clip_0007.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/test/experiment/data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    126\u001b[0m input_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_clips/negative\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[31], line 111\u001b[0m, in \u001b[0;36minit_vgg16\u001b[0;34m(is_acc, video_path, output_path)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(video_name)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# # Combine frame-level and box-level features\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m np\u001b[38;5;241m.\u001b[39msavez_compressed(output_path \u001b[38;5;241m+\u001b[39m video_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mdata, det\u001b[38;5;241m=\u001b[39mdet, labels\u001b[38;5;241m=\u001b[39mlabels, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m)\n\u001b[1;32m    113\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "custom_labels = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "    \"trafficlight\", \"firehydrant\", \"streetsign\", \"stopsign\", \"parkingmeter\", \"bench\", \"bird\",\n",
    "    \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"hat\",\n",
    "    \"backpack\", \"umbrella\", \"shoe\", \"eyeglasses\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "    \"skis\", \"snowboard\", \"sportsball\", \"kite\", \"baseballbat\", \"baseballglove\", \"skateboard\",\n",
    "    \"surfboard\", \"tennisracket\", \"bottle\", \"plate\", \"wineglass\", \"cup\", \"fork\", \"knife\", \"spoon\",\n",
    "    \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hotdog\", \"pizza\",\n",
    "    \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"mirror\", \"diningtable\", \"window\",\n",
    "    \"desk\", \"toilet\", \"door\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cellphone\",\n",
    "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"blender\", \"book\", \"clock\", \"vase\",\n",
    "    \"scissors\", \"teddybear\", \"hairdrier\", \"toothbrush\", \"hairbrush\", \"Van\", \"Truck\", \"Pedestrian\",\n",
    "    \"Person_sitting\", \"Cyclist\", \"Tram\", \"Misc\", \"DontCare\"\n",
    "]\n",
    "\n",
    "# COCO class IDs used by Faster R-CNN (1-91, but you may need to map only those used in your custom set)\n",
    "coco_to_custom_mapping = {\n",
    "    1: \"person\", 2: \"bicycle\", 3: \"car\", 4: \"motorbike\", 5: \"aeroplane\", 6: \"bus\", \n",
    "    7: \"train\", 8: \"truck\", 9: \"boat\", 10: \"trafficlight\", 11: \"firehydrant\", 12: \"streetsign\",\n",
    "    13: \"stopsign\", 14: \"parkingmeter\", 15: \"bench\", 16: \"bird\", 17: \"cat\", 18: \"dog\",\n",
    "    19: \"horse\", 20: \"sheep\", 21: \"cow\", 22: \"elephant\", 23: \"bear\", 24: \"zebra\", 25: \"giraffe\",\n",
    "    26: \"hat\", 27: \"backpack\", 28: \"umbrella\", 29: \"shoe\", 30: \"eyeglasses\", 31: \"handbag\",\n",
    "    32: \"tie\", 33: \"suitcase\", 34: \"frisbee\", 35: \"skis\", 36: \"snowboard\", 37: \"sportsball\",\n",
    "    38: \"kite\", 39: \"baseballbat\", 40: \"baseballglove\", 41: \"skateboard\", 42: \"surfboard\",\n",
    "    43: \"tennisracket\", 44: \"bottle\", 45: \"plate\", 46: \"wineglass\", 47: \"cup\", 48: \"fork\",\n",
    "    49: \"knife\", 50: \"spoon\", 51: \"bowl\", 52: \"banana\", 53: \"apple\", 54: \"sandwich\",\n",
    "    55: \"orange\", 56: \"broccoli\", 57: \"carrot\", 58: \"hotdog\", 59: \"pizza\", 60: \"donut\",\n",
    "    61: \"cake\", 62: \"chair\", 63: \"sofa\", 64: \"pottedplant\", 65: \"bed\", 66: \"mirror\",\n",
    "    67: \"diningtable\", 68: \"window\", 69: \"desk\", 70: \"toilet\", 71: \"door\", 72: \"tvmonitor\",\n",
    "    73: \"laptop\", 74: \"mouse\", 75: \"remote\", 76: \"keyboard\", 77: \"cellphone\", 78: \"microwave\",\n",
    "    79: \"oven\", 80: \"toaster\", 81: \"sink\", 82: \"refrigerator\", 83: \"blender\", 84: \"book\",\n",
    "    85: \"clock\", 86: \"vase\", 87: \"scissors\", 88: \"teddybear\", 89: \"hairdrier\", 90: \"toothbrush\",\n",
    "    91: \"hairbrush\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess image\n",
    "def preprocess_image(frame):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def init_vgg16(is_acc, video_path, output_path):\n",
    "\n",
    "    # Load video\n",
    "    # video_path = './input_clips/positive/clip_0007.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Process frames\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Preprocess the frame\n",
    "        image_tensor = preprocess_image(frame)\n",
    "\n",
    "        # Perform object detection\n",
    "        with torch.no_grad():\n",
    "            predictions = model(image_tensor)\n",
    "        label_names = [];\n",
    "        # Draw bounding boxes\n",
    "        boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "        scores = predictions[0]['scores'].cpu().numpy()\n",
    "        labels = predictions[0]['labels'].cpu().numpy()\n",
    "        \n",
    "        det = []\n",
    "        id = []\n",
    "        frame_features = []\n",
    "        box_features = []\n",
    "        \n",
    "\n",
    "        # Extract frame-level features (using a CNN, here we just flatten the frame as a placeholder)\n",
    "        frame_feature = cv2.resize(frame, (224, 224)).flatten()[:4096]  # Resize to ensure it fits 4096 elements\n",
    "        frame_features.append(frame_feature)\n",
    "\n",
    "        if len(boxes) > 19:\n",
    "            boxes = boxes[:19]  # Ensure only 19 boxes are considered\n",
    "            scores = scores[:19]\n",
    "            labels  = labels[:19]\n",
    "\n",
    "        # for box, score, label in zip(boxes, scores, labels):\n",
    "        #     if score > 0.5 and int(label) in coco_to_custom_mapping:  # Filter for road-specific labels\n",
    "        #         x1, y1, x2, y2 = map(int, box)\n",
    "        #         box_frame = frame[y1:y2, x1:x2]\n",
    "        #         coco_label_id = int(label)\n",
    "        #         det.append([x1, y1, x2, y2, score, coco_label_id])\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2, _, _ = box\n",
    "            box_frame = frame[y1:y2, x1:x2]\n",
    "            if box_frame.size > 0:\n",
    "                box_feature = cv2.resize(box_frame, (64, 64)).flatten()[:4096]\n",
    "                box_features.append(box_feature)\n",
    "            else:\n",
    "                box_features.append(np.zeros(4096))  # In case of invalid box\n",
    "\n",
    "        while len(frame_features) < 100:\n",
    "            frame_features.append(np.zeros(4096))\n",
    "\n",
    "        video_name = video_path.split('/')[-1].split('.')[0]\n",
    "        labels = np.array(is_acc)\n",
    "        id = np.array(video_name)\n",
    "\n",
    "        # # Combine frame-level and box-level features\n",
    "        data = np.concatenate((np.array(frame_features), np.array(box_features)), axis=1)\n",
    "    np.savez_compressed(output_path + video_name + '.npz', data=data, det=det, labels=labels, id=id)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return data, det, labels, id\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'input_clips/positive'\n",
    "output_folder = 'output_npz/positive'\n",
    "\n",
    "\n",
    "init_vgg16([0,1], \"./input_clips/positive/clip_0007.mp4\", \"./data/test/experiment/data/\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'input_clips/negative'\n",
    "output_folder = 'output_npz/negative'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077773d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2046d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class VGG16FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16FeatureExtractor, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = vgg16.features\n",
    "        self.avgpool = vgg16.avgpool\n",
    "        self.classifier = nn.Sequential(*list(vgg16.classifier.children())[:-1])  # remove the last layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Function to preprocess image\n",
    "def preprocess_image(frame):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "def extract_features_from_video(video_path, extractor, frame_count=50, box_count=19):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < frame_count:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        ])\n",
    "        frames.append(transform(frame).unsqueeze(0))\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "# (x1, y1, x2, y2, cls, accident/no acc)bottom left and top right coordinates\n",
    "    # frames = torch.stack(frames)\n",
    "    # frame_features = extractor(frames)\n",
    "    # frame_level_features = frame_features.unsqueeze(1)  # Add dimension for concatenation\n",
    "\n",
    "    # box_level_features = torch.randn(frame_count, box_count, 4096)  # Dummy data for box-level features\n",
    "    # bounding_boxes = torch.randn(frame_count, box_count, 6)  # Dummy data for bounding boxes\n",
    "\n",
    "    data = torch.cat([frame_level_features, box_level_features], dim=1)\n",
    "    return data, bounding_boxes\n",
    "\n",
    "def main(output_path, positive, frames):\n",
    "    video_path = './input_clips/negative/clip_0000.mp4'\n",
    "    # model = VGG16FeatureExtractor()\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.ToPILImage(),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # ])\n",
    "\n",
    "    data, det = extract_features_from_video(video_path, model, frames)\n",
    "    labels = np.array([0, positive])  # Example label, adjust as needed\n",
    "    video_id = os.path.basename(video_path)\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    data_np = data.detach().numpy()\n",
    "    det_np = det.detach().numpy()\n",
    "\n",
    "    # Save data into .npz file\n",
    "    np.savez(output_path, data=data_np, det=det_np, labels=labels, ID=video_id)\n",
    "\n",
    "    print(f'Data shape: {data_np.shape}')  # Should be (100, 20, 4096)\n",
    "    print(f'Detected bounding boxes shape: {det_np.shape}')  # Should be (50, 19, 6)\n",
    "    print(f'Labels: {labels}')\n",
    "    print(f'Video ID: {video_id}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # def process_videos(input_folder, output_folder):\n",
    "    # if not os.path.exists(output_folder):\n",
    "    #     os.makedirs(output_folder)\n",
    "    input_folder = 'input_clips/negative'\n",
    "    output_folder = 'data/test/obj_feat/testing'\n",
    "    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        output_path = os.path.join(output_folder, os.path.splitext(video_file)[0] + '.npz')\n",
    "        main(output_path, 0, 100)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to load YOLO model\n",
    "def load_yolo_model(cfg_path, weights_path, names_path):\n",
    "    net = cv2.dnn.readNet(weights_path, cfg_path)\n",
    "    with open(names_path, 'r') as f:\n",
    "        classes = f.read().strip().split('\\n')\n",
    "    return net, classes\n",
    "\n",
    "# Function to perform object detection on a single frame\n",
    "def detect_objects(net, classes, frame, conf_threshold=0.5, nms_threshold=0.4):\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    detections = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    for output in detections:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > conf_threshold:\n",
    "                center_x = int(detection[0] * w)\n",
    "                center_y = int(detection[1] * h)\n",
    "                width = int(detection[2] * w)\n",
    "                height = int(detection[3] * h)\n",
    "                x = int(center_x - width / 2)\n",
    "                y = int(center_y - height / 2)\n",
    "                boxes.append([x, y, x + width, y + height])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    filtered_boxes = []\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            box = boxes[i]\n",
    "            filtered_boxes.append(box + [confidences[i], class_ids[i]])\n",
    "\n",
    "    return filtered_boxes\n",
    "\n",
    "# Function to process video and extract features and detections\n",
    "def process_video(video_path, yolo_model, classes):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_features = []\n",
    "    detections = []\n",
    "    frame_ids = []\n",
    "    labels = []\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened() and frame_count < 50:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Extract frame-level features (using a CNN, here we just flatten the frame as a placeholder)\n",
    "        frame_feature = cv2.resize(frame, (224, 224)).flatten()[:4096]  # Resize to ensure it fits 4096 elements\n",
    "        frame_features.append(frame_feature)\n",
    "\n",
    "        # Detect objects\n",
    "        boxes = detect_objects(yolo_model, classes, frame)\n",
    "        if len(boxes) > 19:\n",
    "            boxes = boxes[:19]  # Ensure only 19 boxes are considered\n",
    "\n",
    "        # Create box-level features (using the same placeholder approach for now)\n",
    "        box_features = []\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2, _, _ = box\n",
    "            box_frame = frame[y1:y2, x1:x2]\n",
    "            if box_frame.size > 0:\n",
    "                box_feature = cv2.resize(box_frame, (64, 64)).flatten()[:4096]\n",
    "                box_features.append(box_feature)\n",
    "            else:\n",
    "                box_features.append(np.zeros(4096))  # In case of invalid box\n",
    "\n",
    "        while len(box_features) < 19:\n",
    "            box_features.append(np.zeros(4096))  # Pad with zero features if less than 19 boxes\n",
    "\n",
    "        detections.append(boxes)\n",
    "\n",
    "        # Assign a label (placeholder, replace with actual logic)\n",
    "        label = [1, 0] if frame_count % 2 == 0 else [0, 1]  # Replace with actual condition\n",
    "        labels.append(label)\n",
    "\n",
    "        # Collect frame IDs (placeholder, replace with actual video name logic)\n",
    "        frame_ids.append(f'frame_{frame_count}')  # Replace with actual frame ID or video ID logic\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Ensure we have exactly 50 frames\n",
    "    while len(frame_features) < 50:\n",
    "        frame_features.append(np.zeros(4096))\n",
    "        detections.append([[0, 0, 0, 0, 0.0, -1]] * 19)\n",
    "        labels.append([0, 1])\n",
    "        frame_ids.append(f'frame_{len(frame_features)}')\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    frame_features = np.array(frame_features).reshape((50, 1, 4096))\n",
    "    box_features = np.array([np.array(f) for f in detections]).reshape((50, 19, 4096))\n",
    "    detections = np.array(detections)\n",
    "    labels = np.array(labels)\n",
    "    frame_ids = np.array(frame_ids)\n",
    "\n",
    "    # Combine frame-level and box-level features\n",
    "    combined_features = np.concatenate((frame_features, box_features), axis=1)\n",
    "\n",
    "    return combined_features, detections, labels, frame_ids\n",
    "\n",
    "# Paths to YOLO files\n",
    "cfg_path = 'yolov3.cfg'\n",
    "weights_path = 'yolov3.weights'\n",
    "names_path = 'coco.names'\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_model, classes = load_yolo_model(cfg_path, weights_path, names_path)\n",
    "\n",
    "# Process the video\n",
    "video_path = 'input_video.mp4'\n",
    "data, det, labels, ID = process_video(video_path, yolo_model, classes)\n",
    "\n",
    "# Save to .npz file\n",
    "np.savez('output_data.npz', data=data, det=det, labels=labels, ID=ID)\n",
    "\n",
    "print(\"NPZ file successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f47eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0406f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4baf5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65aadaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vt/0x3f4crs5lb36gfgww7p5kb40000gn/T/ipykernel_30489/530300805.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os, sys\n",
    "import os.path as osp\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        VGG = models.vgg16(pretrained=True)\n",
    "        self.feature = VGG.features\n",
    "        self.classifier = nn.Sequential(*list(VGG.classifier.children())[:-3])\n",
    "        pretrained_dict = VGG.state_dict()\n",
    "        model_dict = self.classifier.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.classifier.load_state_dict(model_dict)\n",
    "        self.dim_feat = 4096\n",
    " \n",
    "    def forward(self, x):\n",
    "        output = self.feature(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf0c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_feature_extractor(backbone='vgg16', device=torch.device('cuda')):\n",
    "    feat_extractor = None\n",
    "    if backbone == 'vgg16':\n",
    "        feat_extractor = VGG16()\n",
    "        feat_extractor = feat_extractor.to(device=device)\n",
    "        feat_extractor.eval()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return feat_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4583993",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "MMCV==2.2.0 is used but incompatible. Please install mmcv>=1.3.17, <=1.8.0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_detector, inference_detector, show_result\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmcv\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/mmdet/__init__.py:24\u001b[0m\n\u001b[1;32m     20\u001b[0m mmcv_maximum_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.8.0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m mmcv_version \u001b[38;5;241m=\u001b[39m digit_version(mmcv\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (mmcv_version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m digit_version(mmcv_minimum_version)\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m mmcv_version \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m digit_version(mmcv_maximum_version)), \\\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMMCV==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmmcv\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is used but incompatible. \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease install mmcv>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmmcv_minimum_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, <=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmmcv_maximum_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     29\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort_version\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: MMCV==2.2.0 is used but incompatible. Please install mmcv>=1.3.17, <=1.8.0."
     ]
    }
   ],
   "source": [
    "from mmdet.apis import init_detector, inference_detector, show_result\n",
    "import mmcv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_feature_extractor(backbone='vgg16', device=torch.device('cuda')):\n",
    "    feat_extractor = None\n",
    "    if backbone == 'vgg16':\n",
    "        feat_extractor = VGG16()\n",
    "        feat_extractor = feat_extractor.to(device=device)\n",
    "        feat_extractor.eval()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return feat_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:'+str(p.gpu_id)) if torch.cuda.is_available() else torch.device('cpu')\n",
    "feat_extractor = init_feature_extractor(backbone='vgg16', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe55be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_sampling(bbox_result, nbox=19, imsize=None, topN=5):\n",
    "    \"\"\"\n",
    "    imsize[0]: height\n",
    "    imsize[1]: width\n",
    "    \"\"\"\n",
    "    assert not isinstance(bbox_result, tuple)\n",
    "    bboxes = np.vstack(bbox_result)  # n x 5\n",
    "    labels = [np.full(bbox.shape[0], i, dtype=np.int32) for i, bbox in enumerate(bbox_result)]\n",
    "    labels = np.concatenate(labels)  # n\n",
    "    ndet = bboxes.shape[0]\n",
    "\n",
    "    # fix bbox \n",
    "    new_boxes = []\n",
    "    for box, label in zip(bboxes, labels):\n",
    "        x1 = min(max(0, int(box[0])), imsize[1])\n",
    "        y1 = min(max(0, int(box[1])), imsize[0])\n",
    "        x2 = min(max(x1 + 1, int(box[2])), imsize[1])\n",
    "        y2 = min(max(y1 + 1, int(box[3])), imsize[0])\n",
    "        if (y2 - y1 + 1 > 2) and (x2 - x1 + 1 > 2):\n",
    "            new_boxes.append([x1, y1, x2, y2, box[4], label])\n",
    "    if len(new_boxes) == 0:  # no bboxes\n",
    "        new_boxes.append([0, 0, imsize[1]-1, imsize[0]-1, 1.0, 0])\n",
    "    new_boxes = np.array(new_boxes, dtype=int)\n",
    "    # sampling\n",
    "    n_candidate = min(topN, len(new_boxes))\n",
    "    if len(new_boxes) <= nbox - n_candidate:\n",
    "        indices = np.random.choice(n_candidate, nbox - len(new_boxes), replace=True)\n",
    "        sampled_boxes = np.vstack((new_boxes, new_boxes[indices]))\n",
    "    elif len(new_boxes) > nbox - n_candidate and len(new_boxes) <= nbox:\n",
    "        indices = np.random.choice(n_candidate, nbox - len(new_boxes), replace=False)\n",
    "        sampled_boxes = np.vstack((new_boxes, new_boxes[indices]))\n",
    "    else:\n",
    "        sampled_boxes = new_boxes[:nbox]\n",
    "    \n",
    "    return sampled_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_to_imroi(transform, bboxes, image):\n",
    "    imroi_data = []\n",
    "    for bbox in bboxes:\n",
    "        imroi = image[bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n",
    "        imroi = transform(Image.fromarray(imroi))  # (3, 224, 224), torch.Tensor\n",
    "        imroi_data.append(imroi)\n",
    "    imroi_data = torch.stack(imroi_data)\n",
    "    return imroi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18895234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(detector, feat_extractor, video_file, n_frames=100, n_boxes=19):\n",
    "    assert os.path.join(video_file), video_file\n",
    "    # prepare video reader and data transformer\n",
    "    videoReader = mmcv.VideoReader(video_file)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()]\n",
    "    )\n",
    "    features = np.zeros((n_frames, n_boxes + 1, feat_extractor.dim_feat), dtype=np.float32)\n",
    "    detections = np.zeros((n_frames, n_boxes, 6))  # (50 x 19 x 6)\n",
    "    frame_prev = None\n",
    "    for idx in range(n_frames):\n",
    "        if idx >= len(videoReader):\n",
    "            print(\"Copy frame from previous time step.\")\n",
    "            frame = frame_prev.copy()\n",
    "        else:\n",
    "            frame = videoReader.get_frame(idx)\n",
    "        # run object detection inference\n",
    "        bbox_result = inference_detector(detector, frame)\n",
    "        # sampling a fixed number of bboxes\n",
    "        bboxes = bbox_sampling(bbox_result, nbox=n_boxes, imsize=frame.shape[:2])\n",
    "        detections[idx, :, :] = bboxes\n",
    "        # prepare frame data\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        with torch.no_grad():\n",
    "            # bboxes to roi feature\n",
    "            ims_roi = bbox_to_imroi(transform, bboxes, frame)\n",
    "            ims_roi = ims_roi.float().to(device=device)\n",
    "            feature_roi = feat_extractor(ims_roi)\n",
    "            # extract image feature\n",
    "            ims_frame = transform(Image.fromarray(frame))\n",
    "            ims_frame = torch.unsqueeze(ims_frame, dim=0).float().to(device=device)\n",
    "            feature_frame = feat_extractor(ims_frame)\n",
    "        # obtain feature matrix\n",
    "        features[idx, 0, :] = np.squeeze(feature_frame.cpu().numpy()) if feature_frame.is_cuda else np.squeeze(feature_frame.detach().numpy())\n",
    "        features[idx, 1:, :] = np.squeeze(feature_roi.cpu().numpy()) if feature_roi.is_cuda else np.squeeze(feature_roi.detach().numpy())\n",
    "        frame_prev = frame\n",
    "    return detections, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ead441",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = osp.join(\"lib/mmdetection\", \"/Users/sacithrangana/Desktop/Msc/AAI/gitrepo/Graph_Code/supporting-libs/mmdetection/configs/cascade_rcnn_x101_64x4d_fpn_1x_kitti2d.py\")\n",
    "model_file = osp.join(\"lib/mmdetection\", \"/Users/sacithrangana/Desktop/Msc/AAI/gitrepo/Graph_Code/supporting-libs/mmdetection/work_dirs/cascade_rcnn_x101_64x4d_fpn_1x_kitti2d/latest.pth\")\n",
    "detector = init_detector(cfg_file, model_file, device=device)\n",
    "detections, features = extract_features(detector, feat_extractor, \"data/test/manipulation\", n_frames=100)\n",
    "feat_file = \"1234\"[:-4] + '_feature.npz'\n",
    "np.savez_compressed(feat_file, data=features, det=detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc2c8e-e4af-4414-b3c1-e5e6358af430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to load YOLO model\n",
    "def load_yolo_model(cfg_path, weights_path, names_path):\n",
    "    net = cv2.dnn.readNet(weights_path, cfg_path)\n",
    "    with open(names_path, 'r') as f:\n",
    "        classes = f.read().strip().split('\\n')\n",
    "    return net, classes\n",
    "\n",
    "# Function to perform object detection on a single frame\n",
    "def detect_objects(net, classes, frame, conf_threshold=0.5, nms_threshold=0.4):\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    detections = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    for output in detections:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > conf_threshold:\n",
    "                center_x = int(detection[0] * w)\n",
    "                center_y = int(detection[1] * h)\n",
    "                width = int(detection[2] * w)\n",
    "                height = int(detection[3] * h)\n",
    "                x = int(center_x - width / 2)\n",
    "                y = int(center_y - height / 2)\n",
    "                boxes.append([x, y, x + width, y + height])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    filtered_boxes = []\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            box = boxes[i]\n",
    "            filtered_boxes.append(box + [confidences[i], class_ids[i]])\n",
    "\n",
    "    return filtered_boxes\n",
    "\n",
    "# Function to process video and extract features and detections\n",
    "def process_video(video_path, yolo_model, classes):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_features = []\n",
    "    detections = []\n",
    "    frame_ids = []\n",
    "    labels = []\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened() and frame_count < 50:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Extract frame-level features (using a CNN, here we just flatten the frame as a placeholder)\n",
    "        frame_feature = cv2.resize(frame, (224, 224)).flatten()[:4096]  # Resize to ensure it fits 4096 elements\n",
    "        frame_features.append(frame_feature)\n",
    "\n",
    "        # Detect objects\n",
    "        boxes = detect_objects(yolo_model, classes, frame)\n",
    "        if len(boxes) > 19:\n",
    "            boxes = boxes[:19]  # Ensure only 19 boxes are considered\n",
    "\n",
    "        # Create box-level features (using the same placeholder approach for now)\n",
    "        box_features = []\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2, _, _ = box\n",
    "            box_frame = frame[y1:y2, x1:x2]\n",
    "            if box_frame.size > 0:\n",
    "                box_feature = cv2.resize(box_frame, (64, 64)).flatten()[:4096]\n",
    "                box_features.append(box_feature)\n",
    "            else:\n",
    "                box_features.append(np.zeros(4096))  # In case of invalid box\n",
    "\n",
    "        while len(box_features) < 19:\n",
    "            box_features.append(np.zeros(4096))  # Pad with zero features if less than 19 boxes\n",
    "\n",
    "        detections.append(boxes)\n",
    "\n",
    "        # Assign a label (placeholder, replace with actual logic)\n",
    "        label = [1, 0] if frame_count % 2 == 0 else [0, 1]  # Replace with actual condition\n",
    "        labels.append(label)\n",
    "\n",
    "        # Collect frame IDs (placeholder, replace with actual video name logic)\n",
    "        frame_ids.append(f'frame_{frame_count}')  # Replace with actual frame ID or video ID logic\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Ensure we have exactly 50 frames\n",
    "    while len(frame_features) < 50:\n",
    "        frame_features.append(np.zeros(4096))\n",
    "        detections.append([[0, 0, 0, 0, 0.0, -1]] * 19)\n",
    "        labels.append([0, 1])\n",
    "        frame_ids.append(f'frame_{len(frame_features)}')\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    frame_features = np.array(frame_features).reshape((50, 1, 4096))\n",
    "    box_features = np.array([np.array(f) for f in detections]).reshape((50, 19, 4096))\n",
    "    detections = np.array(detections)\n",
    "    labels = np.array(labels)\n",
    "    frame_ids = np.array(frame_ids)\n",
    "\n",
    "    # Combine frame-level and box-level features\n",
    "    combined_features = np.concatenate((frame_features, box_features), axis=1)\n",
    "\n",
    "    return combined_features, detections, labels, frame_ids\n",
    "\n",
    "# Paths to YOLO files\n",
    "cfg_path = 'yolov3.cfg'\n",
    "weights_path = 'yolov3.weights'\n",
    "names_path = 'coco.names'\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_model, classes = load_yolo_model(cfg_path, weights_path, names_path)\n",
    "\n",
    "# Process the video\n",
    "video_path = 'input_video.mp4'\n",
    "data, det, labels, ID = process_video(video_path, yolo_model, classes)\n",
    "\n",
    "# Save to .npz file\n",
    "np.savez('output_data.npz', data=data, det=det, labels=labels, ID=ID)\n",
    "\n",
    "print(\"NPZ file successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18eaa47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555aba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e1cbfd",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6ea996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility Table:\n",
      "[[5.71809762 6.66122344 7.64206279 8.88265088]\n",
      " [4.97188817 0.         6.65389431 6.58711773]\n",
      " [4.38110824 4.96812224 5.71380863 5.73284128]]\n",
      "Optimal Policy Table:\n",
      "[['up' 'up' 'up' 'left']\n",
      " ['left' None 'left' 'left']\n",
      " ['left' 'up' 'left' 'left']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid world\n",
    "gamma = 0.9\n",
    "reward = np.array([\n",
    "    [-0.04, -0.04, -0.04, +1],\n",
    "    [-0.04, None, -0.04, -1],\n",
    "    [-0.04, -0.04, -0.04, -0.04]\n",
    "])\n",
    "\n",
    "# Possible actions\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_vectors = {\n",
    "    'up': (0, 1),\n",
    "    'down': (0, -1),\n",
    "    'left': (-1, 0),\n",
    "    'right': (1, 0)\n",
    "}\n",
    "\n",
    "# Utility initialization\n",
    "U = np.zeros(reward.shape)\n",
    "\n",
    "# Value iteration function\n",
    "def value_iteration(U, reward, gamma, theta=1e-4):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        U_new = np.copy(U)\n",
    "        for x in range(reward.shape[0]):\n",
    "            for y in range(reward.shape[1]):\n",
    "                if reward[x, y] is None:\n",
    "                    continue\n",
    "                if (x, y) in [(3, 2), (3, 3)]:\n",
    "                    U_new[x, y] = reward[x, y]\n",
    "                else:\n",
    "                    v = []\n",
    "                    for action in actions:\n",
    "                        dx, dy = action_vectors[action]\n",
    "                        nx, ny = x + dx, y + dy\n",
    "                        if 0 <= nx < reward.shape[0] and 0 <= ny < reward.shape[1] and reward[nx, ny] is not None:\n",
    "                            intended = 0.8 * U[nx, ny]\n",
    "                        else:\n",
    "                            intended = 0.8 * U[x, y]\n",
    "                        \n",
    "                        perp1 = perp2 = 0\n",
    "                        if action in ['up', 'down']:\n",
    "                            for a in ['left', 'right']:\n",
    "                                dx, dy = action_vectors[a]\n",
    "                                nx, ny = x + dx, y + dy\n",
    "                                if 0 <= nx < reward.shape[0] and 0 <= ny < reward.shape[1] and reward[nx, ny] is not None:\n",
    "                                    perp1 += 0.1 * U[nx, ny]\n",
    "                                else:\n",
    "                                    perp1 += 0.1 * U[x, y]\n",
    "                        else:\n",
    "                            for a in ['up', 'down']:\n",
    "                                dx, dy = action_vectors[a]\n",
    "                                nx, ny = x + dx, y + dy\n",
    "                                if 0 <= nx < reward.shape[0] and 0 <= ny < reward.shape[1] and reward[nx, ny] is not None:\n",
    "                                    perp2 += 0.1 * U[nx, ny]\n",
    "                                else:\n",
    "                                    perp2 += 0.1 * U[x, y]\n",
    "                        v.append(reward[x, y] + gamma * (intended + perp1 + perp2))\n",
    "                    U_new[x, y] = max(v)\n",
    "                delta = max(delta, abs(U_new[x, y] - U[x, y]))\n",
    "        U = U_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return U\n",
    "\n",
    "# Calculate utilities\n",
    "U = value_iteration(U, reward, gamma)\n",
    "\n",
    "# Print utility table\n",
    "print(\"Utility Table:\")\n",
    "print(U)\n",
    "\n",
    "# Determine optimal policy\n",
    "policy = np.full(reward.shape, None)\n",
    "for x in range(reward.shape[0]):\n",
    "    for y in range(reward.shape[1]):\n",
    "        if reward[x, y] is None or (x, y) in [(3, 2), (3, 3)]:\n",
    "            continue\n",
    "        best_action = None\n",
    "        best_value = -float('inf')\n",
    "        for action in actions:\n",
    "            dx, dy = action_vectors[action]\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < reward.shape[0] and 0 <= ny < reward.shape[1] and reward[nx, ny] is not None:\n",
    "                intended = 0.8 * U[nx, ny]\n",
    "            else:\n",
    "                intended = 0.8 * U[x, y]\n",
    "\n",
    "            perp1 = perp2 = 0\n",
    "            if action in ['up', 'down']:\n",
    "                for a in ['left', 'right']:\n",
    "                    dx, dy = action_vectors[a]\n",
    "                    nx, ny = x + dx, y + dy\n",
    "                    if 0 <= nx < reward.shape[0] and 0 <= ny < reward.shape[1] and reward[nx, ny] is not None:\n",
    "                        perp1 += 0.1 * U[nx, ny]\n",
    "                    else:\n",
    "                        perp1 += 0.1 * U[x, y]\n",
    "            else:\n",
    "                for a in ['up', 'down']:\n",
    "                    dx, dy = action_vectors[a]\n",
    "                    nx, ny = x + dx, y + dy\n",
    "                    if 0 <= nx < reward.shape[0] and 0 <= ny < reward.shape[1] and reward[nx, ny] is not None:\n",
    "                        perp2 += 0.1 * U[nx, ny]\n",
    "                    else:\n",
    "                        perp2 += 0.1 * U[x, y]\n",
    "            value = reward[x, y] + gamma * (intended + perp1 + perp2)\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "        policy[x, y] = best_action\n",
    "\n",
    "# Print policy table\n",
    "print(\"Optimal Policy Table:\")\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffd09fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(0, 1): 797, (0, 0): 105, (1, 0): 98})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define the grid dimensions and rewards\n",
    "grid = [\n",
    "    [-0.04, -0.04, -0.04, +1],\n",
    "    [-0.04, None, -0.04, -1],\n",
    "    [-0.04, -0.04, -0.04, -0.04]\n",
    "]\n",
    "\n",
    "# Possible actions\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_vectors = {\n",
    "    'up': (0, 1),\n",
    "    'down': (0, -1),\n",
    "    'left': (-1, 0),\n",
    "    'right': (1, 0)\n",
    "}\n",
    "\n",
    "# Define the NextState function\n",
    "def next_state(state, action):\n",
    "    x, y = state\n",
    "    # Intended action\n",
    "    dx, dy = action_vectors[action]\n",
    "    intended_state = (x + dx, y + dy)\n",
    "\n",
    "    # Perpendicular actions\n",
    "    perp_actions = {\n",
    "        'up': ['left', 'right'],\n",
    "        'down': ['left', 'right'],\n",
    "        'left': ['up', 'down'],\n",
    "        'right': ['up', 'down']\n",
    "    }\n",
    "    \n",
    "    perp1 = perp_actions[action][0]\n",
    "    perp2 = perp_actions[action][1]\n",
    "\n",
    "    dx1, dy1 = action_vectors[perp1]\n",
    "    dx2, dy2 = action_vectors[perp2]\n",
    "\n",
    "    perp_state1 = (x + dx1, y + dy1)\n",
    "    perp_state2 = (x + dx2, y + dy2)\n",
    "\n",
    "    # Determine next state based on probabilities\n",
    "    rand = random.random()\n",
    "    if rand < 0.8:  # Intended action\n",
    "        next_state = intended_state\n",
    "    elif rand < 0.9:  # Perpendicular action 1\n",
    "        next_state = perp_state1\n",
    "    else:  # Perpendicular action 2\n",
    "        next_state = perp_state2\n",
    "\n",
    "    # Ensure next state is within bounds and not a wall\n",
    "    if not (0 <= next_state[0] < len(grid) and 0 <= next_state[1] < len(grid[0]) and grid[next_state[0]][next_state[1]] is not None):\n",
    "        next_state = state  # Stay in the same state if out of bounds or wall\n",
    "\n",
    "    return next_state\n",
    "\n",
    "# Example usage\n",
    "state = (0, 0)\n",
    "action = 'up'\n",
    "next_states = [next_state(state, action) for _ in range(1000)]\n",
    "\n",
    "# Calculate frequencies of next states\n",
    "from collections import Counter\n",
    "state_counts = Counter(next_states)\n",
    "print(state_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a587046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Transition Probabilities for (state, action) = ((0, 0), 'up'):\n",
      "  (0, 1): 0.800\n",
      "  (0, 0): 0.095\n",
      "  (1, 0): 0.104\n",
      "Estimated Transition Probabilities for (state, action) = ((0, 0), left):\n",
      "  (0, 0): 0.905\n",
      "  (0, 1): 0.095\n",
      "Estimated Transition Probabilities for (state, action) = ((2, 2), right):\n",
      "  (2, 2): 0.805\n",
      "  (2, 1): 0.093\n",
      "  (2, 3): 0.102\n",
      "Estimated Transition Probabilities for (state, action) = ((1, 2), down):\n",
      "  (1, 2): 0.796\n",
      "  (2, 2): 0.098\n",
      "  (0, 2): 0.106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Define the grid dimensions and rewards\n",
    "grid = [\n",
    "    [-0.04, -0.04, -0.04, +1],\n",
    "    [-0.04, None, -0.04, -1],\n",
    "    [-0.04, -0.04, -0.04, -0.04]\n",
    "]\n",
    "\n",
    "# Possible actions\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_vectors = {\n",
    "    'up': (0, 1),\n",
    "    'down': (0, -1),\n",
    "    'left': (-1, 0),\n",
    "    'right': (1, 0)\n",
    "}\n",
    "\n",
    "# Define the NextState function\n",
    "def next_state(state, action):\n",
    "    x, y = state\n",
    "    # Intended action\n",
    "    dx, dy = action_vectors[action]\n",
    "    intended_state = (x + dx, y + dy)\n",
    "\n",
    "    # Perpendicular actions\n",
    "    perp_actions = {\n",
    "        'up': ['left', 'right'],\n",
    "        'down': ['left', 'right'],\n",
    "        'left': ['up', 'down'],\n",
    "        'right': ['up', 'down']\n",
    "    }\n",
    "    \n",
    "    perp1 = perp_actions[action][0]\n",
    "    perp2 = perp_actions[action][1]\n",
    "\n",
    "    dx1, dy1 = action_vectors[perp1]\n",
    "    dx2, dy2 = action_vectors[perp2]\n",
    "\n",
    "    perp_state1 = (x + dx1, y + dy1)\n",
    "    perp_state2 = (x + dx2, y + dy2)\n",
    "\n",
    "    # Determine next state based on probabilities\n",
    "    rand = random.random()\n",
    "    if rand < 0.8:  # Intended action\n",
    "        next_state = intended_state\n",
    "    elif rand < 0.9:  # Perpendicular action 1\n",
    "        next_state = perp_state1\n",
    "    else:  # Perpendicular action 2\n",
    "        next_state = perp_state2\n",
    "\n",
    "    # Ensure next state is within bounds and not a wall\n",
    "    if not (0 <= next_state[0] < len(grid) and 0 <= next_state[1] < len(grid[0]) and grid[next_state[0]][next_state[1]] is not None):\n",
    "        next_state = state  # Stay in the same state if out of bounds or wall\n",
    "\n",
    "    return next_state\n",
    "\n",
    "# Simulate policy execution to learn transition probabilities\n",
    "def simulate_policy(state, action, num_episodes):\n",
    "    transition_counts = defaultdict(lambda: Counter())\n",
    "    for _ in range(num_episodes):\n",
    "        next_s = next_state(state, action)\n",
    "        transition_counts[state, action][next_s] += 1\n",
    "    return transition_counts\n",
    "\n",
    "# Learning transition probabilities\n",
    "num_episodes = 10000\n",
    "transition_counts = simulate_policy((0, 0), 'up', num_episodes)\n",
    "\n",
    "# Calculate transition probabilities\n",
    "transition_probs = {}\n",
    "for (s, a), counts in transition_counts.items():\n",
    "    total_counts = sum(counts.values())\n",
    "    transition_probs[s, a] = {s_prime: count / total_counts for s_prime, count in counts.items()}\n",
    "\n",
    "# Display estimated transition probabilities for (state, action) = ((0, 0), 'up')\n",
    "print(\"Estimated Transition Probabilities for (state, action) = ((0, 0), 'up'):\")\n",
    "for s_prime, prob in transition_probs[(0, 0), 'up'].items():\n",
    "    print(f\"  {s_prime}: {prob:.3f}\")\n",
    "\n",
    "# Simulate for other (state, action) pairs as needed\n",
    "other_pairs = [((0, 0), 'left'), ((2, 2), 'right'), ((1, 2), 'down')]\n",
    "for s, a in other_pairs:\n",
    "    transition_counts = simulate_policy(s, a, num_episodes)\n",
    "    for (s, a), counts in transition_counts.items():\n",
    "        total_counts = sum(counts.values())\n",
    "        transition_probs[s, a] = {s_prime: count / total_counts for s_prime, count in counts.items()}\n",
    "\n",
    "# Display estimated transition probabilities for other (state, action) pairs\n",
    "for s, a in other_pairs:\n",
    "    print(f\"Estimated Transition Probabilities for (state, action) = ({s}, {a}):\")\n",
    "    for s_prime, prob in transition_probs[s, a].items():\n",
    "        print(f\"  {s_prime}: {prob:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f25e499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the grid dimensions and rewards\n",
    "grid = [\n",
    "    [-0.04, -0.04, -0.04, +1],\n",
    "    [-0.04, None, -0.04, -1],\n",
    "    [-0.04, -0.04, -0.04, -0.04]\n",
    "]\n",
    "\n",
    "# Possible actions\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_vectors = {\n",
    "    'up': (0, 1),\n",
    "    'down': (0, -1),\n",
    "    'left': (-1, 0),\n",
    "    'right': (1, 0)\n",
    "}\n",
    "\n",
    "# Define the NextState function\n",
    "def next_state(state, action):\n",
    "    x, y = state\n",
    "    # Intended action\n",
    "    dx, dy = action_vectors[action]\n",
    "    intended_state = (x + dx, y + dy)\n",
    "\n",
    "    # Perpendicular actions\n",
    "    perp_actions = {\n",
    "        'up': ['left', 'right'],\n",
    "        'down': ['left', 'right'],\n",
    "        'left': ['up', 'down'],\n",
    "        'right': ['up', 'down']\n",
    "    }\n",
    "    \n",
    "    perp1 = perp_actions[action][0]\n",
    "    perp2 = perp_actions[action][1]\n",
    "\n",
    "    dx1, dy1 = action_vectors[perp1]\n",
    "    dx2, dy2 = action_vectors[perp2]\n",
    "\n",
    "    perp_state1 = (x + dx1, y + dy1)\n",
    "    perp_state2 = (x + dx2, y + dy2)\n",
    "\n",
    "    # Determine next state based on probabilities\n",
    "    rand = random.random()\n",
    "    if rand < 0.8:  # Intended action\n",
    "        next_state = intended_state\n",
    "    elif rand < 0.9:  # Perpendicular action 1\n",
    "        next_state = perp_state1\n",
    "    else:  # Perpendicular action 2\n",
    "        next_state = perp_state2\n",
    "\n",
    "    # Ensure next state is within bounds and not a wall\n",
    "    if not (0 <= next_state[0] < len(grid) and 0 <= next_state[1] < len(grid[0]) and grid[next_state[0]][next_state[1]] is not None):\n",
    "        next_state = state  # Stay in the same state if out of bounds or wall\n",
    "\n",
    "    return next_state\n",
    "\n",
    "# Define the GLIE scheme\n",
    "def glie_scheme(grid, actions, alpha=0.1, gamma=0.9, episodes=10000):\n",
    "    utilities = np.zeros((len(grid), len(grid[0])))\n",
    "    returns = defaultdict(list)\n",
    "    for episode in range(1, episodes + 1):\n",
    "        state = (0, 0)  # Start state\n",
    "        while state not in [(3, 2), (3, 3)]:\n",
    "            epsilon = 1 / episode  # Decaying epsilon\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(actions)\n",
    "            else:\n",
    "                action = actions[np.argmax([utilities[state[0] + action_vectors[a][0]][state[1] + action_vectors[a][1]] if 0 <= state[0] + action_vectors[a][0] < len(grid) and 0 <= state[1] + action_vectors[a][1] < len(grid[0]) and grid[state[0] + action_vectors[a][0]][state[1] + action_vectors[a][1]] is not None else -float('inf') for a in actions])]\n",
    "            \n",
    "            next_s = next_state(state, action)\n",
    "            reward = grid[next_s[0]][next_s[1]]\n",
    "            utilities[state[0]][state[1]] += alpha * (reward + gamma * utilities[next_s[0]][next_s[1]] - utilities[state[0]][state[1]])\n",
    "            state = next_s\n",
    "\n",
    "    return utilities\n",
    "\n",
    "# Calculate utilities using GLIE scheme\n",
    "utilities = glie_scheme(grid, actions)\n",
    "\n",
    "# Print utility table\n",
    "print(\"Utility Table (GLIE Scheme):\")\n",
    "print(utilities)\n",
    "\n",
    "# Determine optimal policy\n",
    "policy = np.full((len(grid), len(grid[0])), None)\n",
    "for x in range(len(grid)):\n",
    "    for y in range(len(grid[0])):\n",
    "        if grid[x][y] is None or (x, y) in [(3, 2), (3, 3)]:\n",
    "            continue\n",
    "        best_action = None\n",
    "        best_value = -float('inf')\n",
    "        for action in actions:\n",
    "            dx, dy = action_vectors[action]\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < len(grid) and 0 <= ny < len(grid[0]) and grid[nx][ny] is not None:\n",
    "                value = utilities[nx][ny]\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "        policy[x][y] = best_action\n",
    "\n",
    "# Print policy table\n",
    "print(\"Optimal Policy Table (GLIE Scheme):\")\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eedb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d889266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the utility values and policies obtained from Q1 are stored in these variables:\n",
    "utilities_q1 = np.array([\n",
    "    [0.705, 0.762, 0.812, 1.000],\n",
    "    [0.655, None, 0.868, -1.000],\n",
    "    [0.611, 0.659, 0.705, 0.611]\n",
    "])\n",
    "\n",
    "policy_q1 = np.array([\n",
    "    ['right', 'right', 'right', None],\n",
    "    ['up', None, 'up', None],\n",
    "    ['up', 'up', 'up', 'left']\n",
    "])\n",
    "\n",
    "# Utility values and policies obtained from Q4 (GLIE Scheme)\n",
    "utilities_q4 = np.array([\n",
    "    [0.7, 0.76, 0.81, 1.0],\n",
    "    [0.65, None, 0.86, -1.0],\n",
    "    [0.61, 0.66, 0.7, 0.61]\n",
    "])\n",
    "\n",
    "policy_q4 = np.array([\n",
    "    ['right', 'right', 'right', None],\n",
    "    ['up', None, 'up', None],\n",
    "    ['up', 'up', 'up', 'left']\n",
    "])\n",
    "\n",
    "# Display utility tables for comparison\n",
    "print(\"Utility Table (Q1 - Policy/Value Iteration):\")\n",
    "print(utilities_q1)\n",
    "\n",
    "print(\"\\nUtility Table (Q4 - GLIE Scheme):\")\n",
    "print(utilities_q4)\n",
    "\n",
    "# Display policy tables for comparison\n",
    "print(\"\\nOptimal Policy Table (Q1 - Policy/Value Iteration):\")\n",
    "print(policy_q1)\n",
    "\n",
    "print(\"\\nOptimal Policy Table (Q4 - GLIE Scheme):\")\n",
    "print(policy_q4)\n",
    "\n",
    "# Comparison Analysis\n",
    "def compare_utilities(utilities1, utilities2):\n",
    "    diff = np.abs(utilities1 - utilities2)\n",
    "    return diff\n",
    "\n",
    "utility_diff = compare_utilities(utilities_q1, utilities_q4)\n",
    "print(\"\\nDifference in Utility Tables:\")\n",
    "print(utility_diff)\n",
    "\n",
    "def compare_policies(policy1, policy2):\n",
    "    different_policies = np.where(policy1 != policy2)\n",
    "    return different_policies\n",
    "\n",
    "policy_diff_indices = compare_policies(policy_q1, policy_q4)\n",
    "print(\"\\nDifferences in Policy Tables at Indices (row, column):\")\n",
    "print(policy_diff_indices)\n",
    "\n",
    "# Analysis\n",
    "def analyze_convergence(utilities, policy, method):\n",
    "    converged_states = np.isclose(utilities, utilities_q1, atol=0.05)\n",
    "    print(f\"\\nConvergence Analysis for {method}:\")\n",
    "    for x in range(len(utilities)):\n",
    "        for y in range(len(utilities[0])):\n",
    "            if converged_states[x][y]:\n",
    "                print(f\"State ({x+1}, {y+1}) converged with utility {utilities[x][y]:.3f}\")\n",
    "            else:\n",
    "                print(f\"State ({x+1}, {y+1}) did not converge. Utility: {utilities[x][y]:.3f} vs {utilities_q1[x][y]:.3f}\")\n",
    "    \n",
    "    policy_diff = policy_q1 != policy\n",
    "    for x, y in zip(*np.where(policy_diff)):\n",
    "        print(f\"Policy difference at state ({x+1}, {y+1}): {policy_q1[x][y]} vs {policy[x][y]}\")\n",
    "\n",
    "analyze_convergence(utilities_q4, policy_q4, \"GLIE Scheme\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43c1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da40b431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 16:01:52.110992: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load the pre-trained I3D model from TensorFlow Hub\n",
    "model_url = \"https://tfhub.dev/deepmind/i3d-kinetics-400/1\"\n",
    "i3d_model = hub.load(model_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f40f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def video_to_frames(video_path, size=(224, 224)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, size)\n",
    "        frames.append(frame / 255.0)  # Normalize pixel values\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ce4adb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AutoTrackable' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(frames, model)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# For models that use a callable signature\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mserving_default\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m(frames)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# If the above fails, try calling directly if the model supports it\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphofgraph3.9_new_1/lib/python3.9/site-packages/tensorflow/python/saved_model/signature_serialization.py:302\u001b[0m, in \u001b[0;36m_SignatureMap.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 302\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_signatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'serving_default'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     20\u001b[0m video_frames \u001b[38;5;241m=\u001b[39m video_to_frames(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./input_clips/positive/clip_0016.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi3d_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(frames, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m     features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msignatures[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m'\u001b[39m](frames)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# If the above fails, try calling directly if the model supports it\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'AutoTrackable' object is not callable"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(frames, model):\n",
    "    # Convert frames to a tf.Tensor with an extra batch dimension\n",
    "    frames = np.expand_dims(frames, axis=0)\n",
    "    frames = tf.convert_to_tensor(frames, dtype=tf.float32)\n",
    "    \n",
    "    # Use the model by calling the appropriate method or signature\n",
    "    try:\n",
    "        # For models that use a callable signature\n",
    "        features = model.signatures['serving_default'](frames)['default']\n",
    "    except:\n",
    "        # If the above fails, try calling directly if the model supports it\n",
    "        features = model(frames)\n",
    "    \n",
    "    return features.numpy()\n",
    "\n",
    "# Example usage\n",
    "video_frames = video_to_frames('./input_clips/positive/clip_0016.mp4')\n",
    "features = extract_features(video_frames, i3d_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
